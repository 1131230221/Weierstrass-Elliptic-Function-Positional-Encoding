import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import math
from scipy.special import gamma
import timm
from timm.models import vision_transformer
import os
import json
from tqdm import tqdm
import warnings
import time
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import autocast, GradScaler
from timm.data.auto_augment import auto_augment_transform
from timm.data.mixup import Mixup
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class ImprovedWeierstrassEllipticPositionalEncoding(nn.Module):
    """æ”¹è¿›çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯æ¤­åœ†å‡½æ•°ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, num_patches_h, num_patches_w, 
                 use_derivative=True, alpha_scale=0.05, use_layernorm=True):
        super().__init__()
        self.d_model = d_model
        self.num_patches_h = num_patches_h
        self.num_patches_w = num_patches_w
        self.use_derivative = use_derivative
        self.alpha_scale = alpha_scale
        self.use_layernorm = use_layernorm
        
        # è®¡ç®—ç²¾ç¡®çš„åŠå‘¨æœŸ Ï‰1ï¼ˆå¢åŠ æ•°å€¼ç¨³å®šæ€§ï¼‰
        self.omega1 = gamma(0.25)**2 / (2 * np.sqrt(2 * np.pi))
        
        # å¯å­¦ä¹ çš„å‚æ•°ï¼Œä½¿ç”¨æ›´å¥½çš„åˆå§‹åŒ–
        self.alpha_learn = nn.Parameter(torch.tensor(0.5))  # æ›´å¥½çš„åˆå§‹å€¼
        self.beta_learn = nn.Parameter(torch.tensor(0.1))   # é¢å¤–çš„å¯å­¦ä¹ å‚æ•°
        
        # æ”¹è¿›çš„æŠ•å½±å±‚æ¶æ„
        input_dim = 4 if use_derivative else 2
        self.projection = nn.Sequential(
            nn.Linear(input_dim, d_model // 2),
            nn.LayerNorm(d_model // 2) if use_layernorm else nn.Identity(),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, d_model),
            nn.LayerNorm(d_model) if use_layernorm else nn.Identity(),
            nn.GELU(),
            nn.Linear(d_model, d_model)
        )
        
        # å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°
        self.scale_factor = nn.Parameter(torch.tensor(alpha_scale))
        self.offset = nn.Parameter(torch.zeros(1, 1, d_model))
        
        # é¢‘ç‡è°ƒåˆ¶å‚æ•°
        self.freq_modulation = nn.Parameter(torch.ones(4 if use_derivative else 2))
        
        # åˆå§‹åŒ–
        self._init_parameters()
        
    def _init_parameters(self):
        """æ”¹è¿›çš„å‚æ•°åˆå§‹åŒ–"""
        with torch.no_grad():
            for m in self.projection.modules():
                if isinstance(m, nn.Linear):
                    # Xavieråˆå§‹åŒ–ï¼Œä½†ç¼©æ”¾å› å­æ›´å°ï¼Œæé«˜ç¨³å®šæ€§
                    nn.init.xavier_uniform_(m.weight, gain=0.8)
                    nn.init.zeros_(m.bias)
    
    def stable_weierstrass_p(self, z):
        """æ•°å€¼ç¨³å®šçš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯æ¤­åœ†å‡½æ•°è®¡ç®—"""
        # è·å–å­¦ä¹ åˆ°çš„å‚æ•°
        omega2_prime = 0.1 + F.softplus(self.alpha_learn)  # ç¡®ä¿å¤§äº0.1
        beta = 0.01 + F.softplus(self.beta_learn)  # é¢å¤–çš„å½¢çŠ¶å‚æ•°
        
        # é¿å…zæ¥è¿‘0çš„æ•°å€¼é—®é¢˜
        z_real = z.real
        z_imag = z.imag
        
        # è®¡ç®—æ¨¡é•¿ï¼Œé¿å…é™¤é›¶
        z_mod = torch.sqrt(z_real**2 + z_imag**2)
        z_mod_safe = torch.clamp(z_mod, min=1e-6)
        
        # ä¸»é¡¹ï¼š1/z^2ï¼Œä½†åŠ å…¥æ­£åˆ™åŒ–
        main_term = 1.0 / (z_mod_safe**2 + beta)
        
        # å‘¨æœŸä¿®æ­£é¡¹ï¼ˆä½¿ç”¨æ›´ç¨³å®šçš„ä¸‰è§’å‡½æ•°ï¼‰
        u = z_real / self.omega1
        v = z_imag / omega2_prime
        
        # å¤šä¸ªé¢‘ç‡åˆ†é‡çš„å åŠ ï¼Œå¢åŠ å‡½æ•°å¤æ‚æ€§
        correction = torch.zeros_like(z_real)
        for k in range(1, 4):  # ä½¿ç”¨å‰3é¡¹
            freq_factor = k * math.pi
            amp_k = 0.05 / k  # é€’å‡çš„æŒ¯å¹…
            
            correction += amp_k * (
                torch.cos(freq_factor * u) * torch.exp(-freq_factor * torch.abs(v)) +
                torch.sin(freq_factor * v) * torch.exp(-freq_factor * torch.abs(u))
            )
        
        # ç»„åˆä¸»é¡¹å’Œä¿®æ­£é¡¹
        p_z_real = main_term * torch.cos(torch.atan2(z_imag, z_real)) + correction
        p_z_imag = main_term * torch.sin(torch.atan2(z_imag, z_real)) + correction * 0.5
        
        return torch.complex(p_z_real, p_z_imag)
    
    def stable_weierstrass_p_derivative(self, z):
        """æ•°å€¼ç¨³å®šçš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‡½æ•°å¯¼æ•°"""
        omega2_prime = 0.1 + F.softplus(self.alpha_learn)
        beta = 0.01 + F.softplus(self.beta_learn)
        
        z_mod = torch.sqrt(z.real**2 + z.imag**2)
        z_mod_safe = torch.clamp(z_mod, min=1e-6)
        
        # å¯¼æ•°ä¸»é¡¹ï¼š-2/z^3ï¼ŒåŠ å…¥æ­£åˆ™åŒ–
        main_deriv = -2.0 / (z_mod_safe**3 + beta)
        
        # ä¿®æ­£é¡¹çš„å¯¼æ•°
        u = z.real / self.omega1
        v = z.imag / omega2_prime
        
        deriv_correction = torch.zeros_like(z.real)
        for k in range(1, 3):
            freq_factor = k * math.pi
            amp_k = 0.03 / k
            
            deriv_correction += amp_k * k * (
                -torch.sin(freq_factor * u) * torch.exp(-freq_factor * torch.abs(v)) +
                torch.cos(freq_factor * v) * torch.exp(-freq_factor * torch.abs(u))
            )
        
        p_deriv_real = main_deriv * torch.cos(torch.atan2(z.imag, z.real)) + deriv_correction
        p_deriv_imag = main_deriv * torch.sin(torch.atan2(z.imag, z.real)) + deriv_correction * 0.3
        
        return torch.complex(p_deriv_real, p_deriv_imag)
    
    def forward(self, num_patches_h=None, num_patches_w=None):
        """ç”Ÿæˆä½ç½®ç¼–ç """
        if num_patches_h is None:
            num_patches_h = self.num_patches_h
        if num_patches_w is None:
            num_patches_w = self.num_patches_w
            
        device = self.alpha_learn.device
        
        # åˆ›å»ºæ”¹è¿›çš„ç½‘æ ¼åæ ‡ï¼ˆæ·»åŠ å°çš„éšæœºæ‰°åŠ¨ä»¥å¢åŠ é²æ£’æ€§ï¼‰
        row_idx = torch.arange(num_patches_h, dtype=torch.float32, device=device)
        col_idx = torch.arange(num_patches_w, dtype=torch.float32, device=device)
        
        # æ”¹è¿›çš„å½’ä¸€åŒ–æ–¹å¼
        u = (col_idx + 0.5) / (num_patches_w + 1e-8)
        v = (row_idx + 0.5) / (num_patches_h + 1e-8)
        
        # åˆ›å»ºç½‘æ ¼
        u_grid, v_grid = torch.meshgrid(u, v, indexing='xy')
        u_grid = u_grid.T.reshape(-1)
        v_grid = v_grid.T.reshape(-1)
        
        # è·å–å­¦ä¹ å‚æ•°
        omega2_prime = 0.1 + F.softplus(self.alpha_learn)
        
        # æ˜ å°„åˆ°å¤å¹³é¢ï¼Œæ·»åŠ ç›¸ä½åç§»
        z_real = u_grid * self.omega1 + 0.01 * torch.sin(2 * math.pi * u_grid)
        z_imag = v_grid * omega2_prime + 0.01 * torch.cos(2 * math.pi * v_grid)
        z = torch.complex(z_real, z_imag)
        
        # è®¡ç®—å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‡½æ•°å€¼
        p_z = self.stable_weierstrass_p(z)
        
        # åˆ†ç¦»å®éƒ¨å’Œè™šéƒ¨ï¼Œåº”ç”¨é¢‘ç‡è°ƒåˆ¶
        p_real = p_z.real * self.freq_modulation[0]
        p_imag = p_z.imag * self.freq_modulation[1]
        
        # æ”¹è¿›çš„æ¿€æ´»å‡½æ•°ç»„åˆ
        pe_real = torch.tanh(self.scale_factor * p_real)
        pe_imag = torch.tanh(self.scale_factor * p_imag * 0.8)
        
        if self.use_derivative:
            # è®¡ç®—å¯¼æ•°
            p_prime = self.stable_weierstrass_p_derivative(z)
            p_prime_real = torch.tanh(self.scale_factor * p_prime.real * self.freq_modulation[2])
            p_prime_imag = torch.tanh(self.scale_factor * p_prime.imag * self.freq_modulation[3])
            
            # ç»„åˆ4ç»´ç‰¹å¾
            features = torch.stack([pe_real, pe_imag, p_prime_real, p_prime_imag], dim=-1)
        else:
            # ç»„åˆ2ç»´ç‰¹å¾
            features = torch.stack([pe_real, pe_imag], dim=-1)
        
        # æŠ•å½±åˆ°d_modelç»´
        pos_encoding = self.projection(features)
        
        # æ·»åŠ å¯å­¦ä¹ çš„åç§»
        pos_encoding = pos_encoding + self.offset
        
        # ä¿®æ­£è¾“å‡º shapeï¼Œç¡®ä¿ä¸º (num_patches, d_model)
        if pos_encoding.dim() == 3 and pos_encoding.shape[0] == 1:
            pos_encoding = pos_encoding.squeeze(0)
        elif pos_encoding.dim() > 2:
            pos_encoding = pos_encoding.view(-1, self.d_model)
        return pos_encoding

class ImprovedVisionTransformerWithWeierstrassEncoding(nn.Module):
    """æ”¹è¿›çš„ä½¿ç”¨å¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç çš„Vision Transformer"""
    
    def __init__(self, base_model, image_size=224, patch_size=16, num_classes=100,
                 use_derivative=True, alpha_scale=0.05, use_stochastic_depth=True):
        super().__init__()
        
        self.base_model = base_model
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2
        self.num_patches_h = image_size // patch_size
        self.num_patches_w = image_size // patch_size
        
        # è·å–æ¨¡å‹ç»´åº¦
        self.d_model = base_model.embed_dim
        
        # æ›¿æ¢ä½ç½®ç¼–ç 
        self.weierstrass_encoding = ImprovedWeierstrassEllipticPositionalEncoding(
            d_model=self.d_model,
            num_patches_h=self.num_patches_h,
            num_patches_w=self.num_patches_w,
            use_derivative=use_derivative,
            alpha_scale=alpha_scale,
            use_layernorm=True
        )
        
        # æ”¹è¿›çš„CLS tokenä½ç½®ç¼–ç 
        self.cls_pos_encoding = nn.Parameter(torch.zeros(1, 1, self.d_model))
        nn.init.normal_(self.cls_pos_encoding, std=0.01)
        
        # æ·»åŠ ä½ç½®ç¼–ç çš„dropout
        self.pos_dropout = nn.Dropout(0.1)
        
        # éšæœºæ·±åº¦ï¼ˆStochastic Depthï¼‰
        if use_stochastic_depth:
            self.enable_stochastic_depth()
        
        # æ”¹è¿›çš„åˆ†ç±»å¤´
        self.pre_classifier = nn.Sequential(
            nn.LayerNorm(self.d_model),
            nn.Dropout(0.3)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(self.d_model // 2, num_classes)
        )
        
        # åˆå§‹åŒ–åˆ†ç±»å™¨
        self._init_classifier()
        
    def enable_stochastic_depth(self):
        """å¯ç”¨éšæœºæ·±åº¦"""
        drop_path_rate = 0.1
        num_layers = len(self.base_model.blocks)
        drop_path_rates = [x.item() for x in torch.linspace(0, drop_path_rate, num_layers)]
        
        for i, block in enumerate(self.base_model.blocks):
            if hasattr(block, 'drop_path'):
                block.drop_path.drop_prob = drop_path_rates[i]
        
    def _init_classifier(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.8)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        B = x.shape[0]
        
        # Patch embedding
        x = self.base_model.patch_embed(x)  # (B, num_patches, D)
        
        # æ·»åŠ CLS token
        cls_tokens = self.base_model.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)  # (B, 1+num_patches, D)
        
        # è·å–å¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç 
        patch_pos_encoding = self.weierstrass_encoding()  # (num_patches, D)
        
        # ç»„åˆCLSå’Œpatchçš„ä½ç½®ç¼–ç 
        pos_encoding = torch.cat([
            self.cls_pos_encoding,  # (1, 1, D)
            patch_pos_encoding.unsqueeze(0)  # (1, num_patches, D)
        ], dim=1)  # (1, 1+num_patches, D)
        
        # æ·»åŠ ä½ç½®ç¼–ç å’Œdropout
        x = x + pos_encoding
        x = self.pos_dropout(x)
        
        # Transformer blocks
        x = self.base_model.pos_drop(x)
        x = self.base_model.blocks(x)
        x = self.base_model.norm(x)
        
        # åˆ†ç±»
        x = x[:, 0]  # å–CLS token
        x = self.pre_classifier(x)
        x = self.classifier(x)
        
        return x

# æ”¹è¿›çš„æ•°æ®å¢å¼º
def get_improved_cifar100_dataloaders(batch_size=64, num_workers=4, distributed=False, rank=0, world_size=1):
    """è·å–æ”¹è¿›çš„CIFAR-100æ•°æ®åŠ è½½å™¨"""
    
    # æ›´å¼ºçš„æ•°æ®å¢å¼º
    transform_train = transforms.Compose([
        transforms.Resize(256),  # å…ˆæ”¾å¤§
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.9, 1.1)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
        transforms.RandomGrayscale(p=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],
                           std=[0.2675, 0.2565, 0.2761]),
        transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3)),
    ])
    
    transform_test = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],
                           std=[0.2675, 0.2565, 0.2761]),
    ])
    
    # åŠ è½½æ•°æ®é›†
    train_dataset = datasets.CIFAR100(root='./data', train=True, 
                                     download=True, transform=transform_train)
    test_dataset = datasets.CIFAR100(root='./data', train=False, 
                                    download=True, transform=transform_test)
    
    if distributed:
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
        test_sampler = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank)
    else:
        train_sampler = None
        test_sampler = None
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                            shuffle=(train_sampler is None),
                            num_workers=num_workers, 
                            pin_memory=True,
                            sampler=train_sampler,
                            drop_last=True)  # æ·»åŠ drop_last
    
    test_loader = DataLoader(test_dataset, batch_size=batch_size, 
                           shuffle=False,
                           num_workers=num_workers, 
                           pin_memory=True,
                           sampler=test_sampler)
    
    return train_loader, test_loader, train_sampler

# Mixupæ•°æ®å¢å¼º
def setup_mixup(mixup_alpha=0.2, cutmix_alpha=1.0, prob=0.5):
    """è®¾ç½®Mixupå’ŒCutMix"""
    return Mixup(
        mixup_alpha=mixup_alpha, cutmix_alpha=cutmix_alpha, cutmix_minmax=None,
        prob=prob, switch_prob=0.5, mode='batch',
        label_smoothing=0.1, num_classes=100
    )

def train_epoch_improved(model, train_loader, criterion, optimizer, device, epoch, 
                        train_sampler=None, scaler=None, mixup_fn=None):
    """æ”¹è¿›çš„è®­ç»ƒå‡½æ•°"""
    model.train()
    if train_sampler is not None:
        train_sampler.set_epoch(epoch)
        
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
    for batch_idx, (inputs, targets) in enumerate(pbar):
        inputs, targets = inputs.to(device), targets.to(device)
        
        # åº”ç”¨Mixup/CutMix
        if mixup_fn is not None:
            inputs, targets = mixup_fn(inputs, targets)
        
        optimizer.zero_grad()
        
        # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        with autocast():
            outputs = model(inputs)
            if mixup_fn is not None:
                loss = criterion(outputs, targets)
            else:
                loss = criterion(outputs, targets)
        
        if scaler is not None:
            scaler.scale(loss).backward()
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        
        running_loss += loss.item()
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¯¹äºmixupéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        if mixup_fn is None:
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
        else:
            # Mixupæƒ…å†µä¸‹ä½¿ç”¨è½¯æ ‡ç­¾çš„è¿‘ä¼¼å‡†ç¡®ç‡
            _, predicted = outputs.max(1)
            if hasattr(targets, 'argmax'):
                targets_hard = targets.argmax(1)
            else:
                targets_hard = targets
            total += targets_hard.size(0)
            correct += predicted.eq(targets_hard).sum().item()
        
        # æ›´æ–°è¿›åº¦æ¡
        if total > 0:
            pbar.set_postfix({
                'Loss': f'{running_loss/(batch_idx+1):.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        # å®šæœŸæ¸…ç†ç¼“å­˜
        if batch_idx % 10 == 0:
            torch.cuda.empty_cache()
    
    return running_loss / len(train_loader), 100. * correct / total if total > 0 else 0

def evaluate_improved(model, test_loader, criterion, device):
    """æ”¹è¿›çš„è¯„ä¼°å‡½æ•°"""
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in tqdm(test_loader, desc='Evaluating'):
            inputs, targets = inputs.to(device), targets.to(device)
            
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
            
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            torch.cuda.empty_cache()
    
    return test_loss / len(test_loader), 100. * correct / total

def main():
    # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        gpu = int(os.environ['LOCAL_RANK'])
        dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)
        torch.cuda.set_device(gpu)
        distributed = True
    else:
        distributed = False
        rank = 0
        world_size = 1
        gpu = 0
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')
    if rank == 0:
        print(f"Using device: {device}")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42 + rank)
    
    # æ”¹è¿›çš„è¶…å‚æ•°
    batch_size = 20  # ç¨å¾®å¢åŠ batch size
    learning_rate = 0.0008  # ç•¥å¾®é™ä½å­¦ä¹ ç‡
    num_epochs = 35  # å¢åŠ epochæ•°
    warmup_epochs = 8  # å¢åŠ warmup
    weight_decay = 0.05  # å¢åŠ æ­£åˆ™åŒ–
    num_workers = 11
    
    # GPUå†…å­˜è®¾ç½®
    torch.cuda.set_per_process_memory_fraction(0.85)
    torch.cuda.empty_cache()
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    if rank == 0:
        print("Loading pretrained ViT-B/16 model...")
    
    try:
        base_model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=0)
        pretrained_path = 'jx_vit_base_patch16_224_in21k-e5005f0a.pth'
        
        state_dict = torch.load(pretrained_path, map_location='cpu')
        if 'model' in state_dict:
            state_dict = state_dict['model']
        
        keys_to_remove = ['pre_logits.fc.bias', 'pre_logits.fc.weight', 'head.bias', 'head.weight']
        for key in keys_to_remove:
            if key in state_dict:
                del state_dict[key]
        
        base_model.load_state_dict(state_dict, strict=False)
        if rank == 0:
            print("Successfully loaded pretrained model")
    except Exception as e:
        if rank == 0:
            print(f"Error loading pretrained model: {str(e)}")
            print("Using untrained model...")
        base_model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=0)
    
    # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
    model = ImprovedVisionTransformerWithWeierstrassEncoding(
        base_model=base_model,
        image_size=224,
        patch_size=16,
        num_classes=100,
        use_derivative=True,
        alpha_scale=0.05,
        use_stochastic_depth=True
    )
    
    model = model.to(device)
    
    if distributed:
        model = DDP(model, device_ids=[gpu], find_unused_parameters=True)
    
    # è·å–æ”¹è¿›çš„æ•°æ®åŠ è½½å™¨
    train_loader, test_loader, train_sampler = get_improved_cifar100_dataloaders(
        batch_size=batch_size,
        num_workers=num_workers,
        distributed=distributed,
        rank=rank,
        world_size=world_size
    )
    
    # è®¾ç½®Mixup
    mixup_fn = setup_mixup(mixup_alpha=0.2, cutmix_alpha=1.0, prob=0.5)
    
    # æ”¹è¿›çš„æŸå¤±å‡½æ•°ï¼ˆæ ‡ç­¾å¹³æ»‘ï¼‰
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    # æ”¹è¿›çš„ä¼˜åŒ–å™¨è®¾ç½®
    if distributed:
        base_params = []
        new_params = []
        weierstrass_params = []
        
        for name, param in model.module.named_parameters():
            if 'weierstrass_encoding' in name:
                weierstrass_params.append(param)
            elif 'cls_pos_encoding' in name or 'classifier' in name or 'pre_classifier' in name:
                new_params.append(param)
            else:
                base_params.append(param)
    else:
        base_params = []
        new_params = []
        weierstrass_params = []
        
        for name, param in model.named_parameters():
            if 'weierstrass_encoding' in name:
                weierstrass_params.append(param)
            elif 'cls_pos_encoding' in name or 'classifier' in name or 'pre_classifier' in name:
                new_params.append(param)
            else:
                base_params.append(param)
    
    # ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
    optimizer = optim.AdamW([
        {'params': base_params, 'lr': learning_rate * 0.05},  # é¢„è®­ç»ƒå‚æ•°æ›´å°å­¦ä¹ ç‡
        {'params': weierstrass_params, 'lr': learning_rate * 0.8},  # å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‚æ•°
        {'params': new_params, 'lr': learning_rate}  # æ–°å‚æ•°æ­£å¸¸å­¦ä¹ ç‡
    ], weight_decay=weight_decay, betas=(0.9, 0.999), eps=1e-8)
    
    # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨
    scaler = GradScaler()
    
    # æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
    def improved_lr_lambda(epoch):
        if epoch < warmup_epochs:
            return epoch / warmup_epochs
        else:
            # ä½™å¼¦é€€ç« + é‡å¯
            cycle_epoch = (epoch - warmup_epochs) % 15
            cycle_progress = cycle_epoch / 15
            return 0.5 * (1 + math.cos(math.pi * cycle_progress)) * 0.95 ** ((epoch - warmup_epochs) // 15)
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, improved_lr_lambda)
    
    # è®­ç»ƒå¾ªç¯
    best_acc = 0.0
    results = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}
    patience = 8
    patience_counter = 0
    
    if rank == 0:
        print("\nStarting improved training...")
    
    for epoch in range(1, num_epochs + 1):
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch_improved(
            model, train_loader, criterion, optimizer, device, epoch, 
            train_sampler, scaler, mixup_fn if epoch > 5 else None  # å‰5ä¸ªepochä¸ç”¨mixup
        )
        
        # è¯„ä¼°
        test_loss, test_acc = evaluate_improved(model, test_loader, criterion, device)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•ç»“æœ
        results['train_loss'].append(train_loss)
        results['train_acc'].append(train_acc)
        results['test_loss'].append(test_loss)
        results['test_acc'].append(test_acc)
        
        if rank == 0:
            print(f"\nEpoch {epoch}/{num_epochs}:")
            print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")
            print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
            
            # æ—©åœå’Œæœ€ä½³æ¨¡å‹ä¿å­˜
            if test_acc > best_acc:
                best_acc = test_acc
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                model_state = model.module.state_dict() if distributed else model.state_dict()
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model_state,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'best_acc': best_acc,
                    'results': results
                }, 'improved_weierstrass_vit_best.pth')
                
                print(f"ğŸ‰ New best accuracy: {best_acc:.2f}%")
                
                # å¦‚æœè¶…è¿‡ç›®æ ‡å‡†ç¡®ç‡ï¼Œè®°å½•
                if best_acc > 91.67:
                    print(f"ğŸš€ Target achieved! Best accuracy: {best_acc:.2f}% > 91.67%")
            else:
                patience_counter += 1
                if patience_counter >= patience and epoch > 20:
                    print(f"Early stopping triggered after {epoch} epochs")
                    break
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 5 == 0:
                model_state = model.module.state_dict() if distributed else model.state_dict()
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model_state,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'results': results
                }, f'checkpoint_epoch_{epoch}.pth')
    
    if rank == 0:
        print(f"\nğŸ Training completed. Best accuracy: {best_acc:.2f}%")
        if best_acc > 91.67:
            print(f"ğŸ¯ Successfully exceeded baseline of 91.67%!")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        with open('improved_training_results.json', 'w') as f:
            json.dump(results, f, indent=4)
        
        # è¾“å‡ºå­¦ä¹ åˆ°çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‚æ•°
        if distributed:
            alpha = F.softplus(model.module.weierstrass_encoding.alpha_learn).item()
            beta = F.softplus(model.module.weierstrass_encoding.beta_learn).item()
        else:
            alpha = F.softplus(model.weierstrass_encoding.alpha_learn).item()
            beta = F.softplus(model.weierstrass_encoding.beta_learn).item()
        
        print(f"\nLearned Weierstrass parameters:")
        print(f"Ï‰'2 (alpha): {alpha:.6f}")
        print(f"Î² (beta): {beta:.6f}")
    
    if distributed:
        dist.destroy_process_group()

if __name__ == "__main__":
    main()