import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset, Dataset
from torchvision import datasets, transforms
import numpy as np
import math
from scipy.special import gamma
import os
import json
from tqdm import tqdm
import warnings
import time
import random
from transformers import ViTModel, ViTConfig
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import autocast, GradScaler
import scipy.io as sio
from PIL import Image
import os
warnings.filterwarnings('ignore')

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥è§£å†³å¤šè¿›ç¨‹é—®é¢˜
os.environ['TMPDIR'] = '/tmp'
os.environ['TEMP'] = '/tmp'
os.environ['TMP'] = '/tmp'

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿å®éªŒå¯é‡å¤æ€§"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class ImprovedWeierstrassEllipticPositionalEncoding(nn.Module):
    """æ”¹è¿›çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯æ¤­åœ†å‡½æ•°ä½ç½®ç¼–ç  - é’ˆå¯¹ViT-L/16ä¼˜åŒ–"""
    
    def __init__(self, d_model, num_patches_h, num_patches_w, 
                 use_derivative=True, alpha_scale=0.03, use_layernorm=True):
        super().__init__()
        self.d_model = d_model
        self.num_patches_h = num_patches_h
        self.num_patches_w = num_patches_w
        self.use_derivative = use_derivative
        self.alpha_scale = alpha_scale
        self.use_layernorm = use_layernorm
        
        # å¨å°”æ–¯ç‰¹æ‹‰æ–¯æ¤­åœ†å‡½æ•°çš„ç²¾ç¡®åŠå‘¨æœŸ
        self.omega1 = gamma(0.25)**2 / (2 * np.sqrt(2 * np.pi))
        
        # å¯å­¦ä¹ å‚æ•°
        self.alpha_learn = nn.Parameter(torch.tensor(0.5))
        self.beta_learn = nn.Parameter(torch.tensor(0.1))
        self.gamma_learn = nn.Parameter(torch.tensor(0.2))  # æ–°å¢å‚æ•°
        
        # å¤šå±‚æŠ•å½±ç½‘ç»œ
        input_dim = 6 if use_derivative else 3  # å¢åŠ ç»´åº¦
        self.projection = nn.Sequential(
            nn.Linear(input_dim, d_model // 2),
            nn.LayerNorm(d_model // 2) if use_layernorm else nn.Identity(),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, d_model),
            nn.LayerNorm(d_model) if use_layernorm else nn.Identity(),
            nn.GELU(),
            nn.Dropout(0.05),
            nn.Linear(d_model, d_model)
        )
        
        # å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»
        self.scale_factor = nn.Parameter(torch.tensor(alpha_scale))
        self.offset = nn.Parameter(torch.zeros(1, 1, d_model))
        
        # é¢‘ç‡è°ƒåˆ¶å‚æ•°
        freq_dim = 6 if use_derivative else 3
        self.freq_modulation = nn.Parameter(torch.ones(freq_dim))
        
        # åˆå§‹åŒ–
        self._init_parameters()
        
    def _init_parameters(self):
        """Xavieråˆå§‹åŒ–"""
        with torch.no_grad():
            for m in self.projection.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight, gain=0.8)
                    nn.init.zeros_(m.bias)
    
    def stable_weierstrass_p(self, z):
        """æ•°å€¼ç¨³å®šçš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯æ¤­åœ†å‡½æ•°"""
        omega2_prime = 0.1 + F.softplus(self.alpha_learn)
        beta = 0.01 + F.softplus(self.beta_learn)
        gamma_param = 0.1 + F.softplus(self.gamma_learn)
        
        z_real = z.real
        z_imag = z.imag
        z_mod = torch.sqrt(z_real**2 + z_imag**2)
        z_mod_safe = torch.clamp(z_mod, min=1e-6)
        
        # ä¸»é¡¹
        main_term = 1.0 / (z_mod_safe**2 + beta)
        
        # å‘¨æœŸä¿®æ­£é¡¹
        u = z_real / self.omega1
        v = z_imag / omega2_prime
        
        correction = torch.zeros_like(z_real)
        for k in range(1, 4):
            freq_factor = k * math.pi
            amp_k = gamma_param / k**2
            
            correction += amp_k * (
                torch.cos(freq_factor * u) * torch.exp(-freq_factor * torch.abs(v)) +
                torch.sin(freq_factor * v) * torch.exp(-freq_factor * torch.abs(u))
            )
        
        p_z_real = main_term * torch.cos(torch.atan2(z_imag, z_real)) + correction
        p_z_imag = main_term * torch.sin(torch.atan2(z_imag, z_real)) + correction * 0.7
        
        return torch.complex(p_z_real, p_z_imag)
    
    def stable_weierstrass_p_derivative(self, z):
        """å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‡½æ•°å¯¼æ•°"""
        omega2_prime = 0.1 + F.softplus(self.alpha_learn)
        beta = 0.01 + F.softplus(self.beta_learn)
        gamma_param = 0.1 + F.softplus(self.gamma_learn)
        
        z_mod = torch.sqrt(z.real**2 + z.imag**2)
        z_mod_safe = torch.clamp(z_mod, min=1e-6)
        
        main_deriv = -2.0 / (z_mod_safe**3 + beta)
        
        u = z.real / self.omega1
        v = z.imag / omega2_prime
        
        deriv_correction = torch.zeros_like(z.real)
        for k in range(1, 3):
            freq_factor = k * math.pi
            amp_k = gamma_param / k**2
            
            deriv_correction += amp_k * k * (
                -torch.sin(freq_factor * u) * torch.exp(-freq_factor * torch.abs(v)) +
                torch.cos(freq_factor * v) * torch.exp(-freq_factor * torch.abs(u))
            )
        
        p_deriv_real = main_deriv * torch.cos(torch.atan2(z.imag, z.real)) + deriv_correction
        p_deriv_imag = main_deriv * torch.sin(torch.atan2(z.imag, z.real)) + deriv_correction * 0.5
        
        return torch.complex(p_deriv_real, p_deriv_imag)
    
    def forward(self, num_patches_h=None, num_patches_w=None):
        """ç”Ÿæˆä½ç½®ç¼–ç """
        if num_patches_h is None:
            num_patches_h = self.num_patches_h
        if num_patches_w is None:
            num_patches_w = self.num_patches_w
            
        device = self.alpha_learn.device
        
        # åˆ›å»ºç½‘æ ¼åæ ‡
        row_idx = torch.arange(num_patches_h, dtype=torch.float32, device=device)
        col_idx = torch.arange(num_patches_w, dtype=torch.float32, device=device)
        
        # æ”¹è¿›çš„å½’ä¸€åŒ–
        u = (col_idx + 0.5) / (num_patches_w + 1e-8)
        v = (row_idx + 0.5) / (num_patches_h + 1e-8)
        
        u_grid, v_grid = torch.meshgrid(u, v, indexing='xy')
        u_grid = u_grid.T.reshape(-1)
        v_grid = v_grid.T.reshape(-1)
        
        omega2_prime = 0.1 + F.softplus(self.alpha_learn)
        
        # æ˜ å°„åˆ°å¤å¹³é¢ï¼Œå¢åŠ éçº¿æ€§å˜æ¢
        z_real = u_grid * self.omega1 + 0.02 * torch.sin(3 * math.pi * u_grid)
        z_imag = v_grid * omega2_prime + 0.02 * torch.cos(3 * math.pi * v_grid)
        z = torch.complex(z_real, z_imag)
        
        # è®¡ç®—å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‡½æ•°å€¼
        p_z = self.stable_weierstrass_p(z)
        
        # åˆ†ç¦»å®éƒ¨å’Œè™šéƒ¨
        p_real = p_z.real * self.freq_modulation[0]
        p_imag = p_z.imag * self.freq_modulation[1]
        
        # æ”¹è¿›çš„æ¿€æ´»å‡½æ•°
        pe_real = torch.tanh(self.scale_factor * p_real)
        pe_imag = torch.tanh(self.scale_factor * p_imag * 0.8)
        
        # æ·»åŠ å¾„å‘åˆ†é‡
        radius = torch.sqrt(u_grid**2 + v_grid**2)
        pe_radius = torch.tanh(self.scale_factor * radius * self.freq_modulation[2])
        
        if self.use_derivative:
            p_prime = self.stable_weierstrass_p_derivative(z)
            p_prime_real = torch.tanh(self.scale_factor * p_prime.real * self.freq_modulation[3])
            p_prime_imag = torch.tanh(self.scale_factor * p_prime.imag * self.freq_modulation[4])
            p_prime_radius = torch.tanh(self.scale_factor * p_prime.real.abs() * self.freq_modulation[5])
            
            features = torch.stack([pe_real, pe_imag, pe_radius, 
                                  p_prime_real, p_prime_imag, p_prime_radius], dim=-1)
        else:
            features = torch.stack([pe_real, pe_imag, pe_radius], dim=-1)
        
        # æŠ•å½±åˆ°d_modelç»´
        pos_encoding = self.projection(features)
        pos_encoding = pos_encoding + self.offset
        
        return pos_encoding.squeeze(0) if pos_encoding.dim() == 3 else pos_encoding

class WeierstrassViTL16(nn.Module):
    """åŸºäºæœ¬åœ°é¢„è®­ç»ƒViT-L/16çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç æ¨¡å‹"""
    
    def __init__(self, pretrained_model_path="/root/shared-nvme/vit_l16_in21k.pth", 
                 num_classes=10, image_size=384, use_derivative=True, alpha_scale=0.03):
        super().__init__()
        
        # åˆ›å»ºViT-L/16é…ç½®ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰- æ”¯æŒ384Ã—384åˆ†è¾¨ç‡
        self.config = ViTConfig(
            hidden_size=1024,
            num_hidden_layers=24,
            num_attention_heads=16,
            intermediate_size=4096,
            hidden_dropout_prob=0.0,
            attention_probs_dropout_prob=0.0,
            initializer_range=0.02,
            layer_norm_eps=1e-12,
            image_size=image_size,  # ä½¿ç”¨ä¼ å…¥çš„image_size
            patch_size=16,
            num_channels=3,
            qkv_bias=True,
            encoder_stride=16,
        )
        
        # è®¡ç®—patchæ•°é‡
        self.image_size = image_size
        self.patch_size = self.config.patch_size
        self.num_patches_per_side = image_size // self.patch_size
        self.num_patches = self.num_patches_per_side ** 2
        
        # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if os.path.exists(pretrained_model_path):
            print(f"Loading pretrained model from: {pretrained_model_path}")
            
            # åŠ è½½æœ¬åœ°æƒé‡
            checkpoint = torch.load(pretrained_model_path, map_location='cpu')
            if 'model_state_dict' in checkpoint:
                # å¦‚æœæ˜¯å®Œæ•´çš„checkpointæ ¼å¼
                state_dict = checkpoint['model_state_dict']
            else:
                # å¦‚æœæ˜¯ç›´æ¥çš„state_dictæ ¼å¼
                state_dict = checkpoint
            
            # åˆ›å»ºViTæ¨¡å‹å®ä¾‹
            self.vit = ViTModel(self.config, add_pooling_layer=False)
            
            # å¤„ç†ä½ç½®ç¼–ç å°ºå¯¸ä¸åŒ¹é…é—®é¢˜
            if 'embeddings.position_embeddings' in state_dict:
                old_pos_embed = state_dict['embeddings.position_embeddings']
                old_size = old_pos_embed.shape[1]  # åº”è¯¥æ˜¯197 (224x224)
                new_size = self.num_patches + 1    # åº”è¯¥æ˜¯577 (384x384)
                
                if old_size != new_size:
                    print(f"Resizing position embeddings from {old_size} to {new_size}")
                    
                    # è·å–CLS tokençš„ä½ç½®ç¼–ç 
                    cls_pos_embed = old_pos_embed[:, 0:1, :]
                    
                    # è·å–patchä½ç½®ç¼–ç 
                    patch_pos_embed = old_pos_embed[:, 1:, :]
                    
                    # é‡å¡‘patchä½ç½®ç¼–ç ä»¥åŒ¹é…æ–°çš„å°ºå¯¸
                    old_patch_size = int(np.sqrt(old_size - 1))  # 14 (224/16)
                    new_patch_size = int(np.sqrt(new_size - 1))  # 24 (384/16)
                    
                    # é‡å¡‘ä¸º2Dç½‘æ ¼
                    patch_pos_embed = patch_pos_embed.reshape(1, old_patch_size, old_patch_size, -1)
                    
                    # ä½¿ç”¨åŒçº¿æ€§æ’å€¼è°ƒæ•´å°ºå¯¸
                    patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)  # (1, D, H, W)
                    patch_pos_embed = F.interpolate(
                        patch_pos_embed, 
                        size=(new_patch_size, new_patch_size), 
                        mode='bilinear', 
                        align_corners=False
                    )
                    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1)  # (1, H, W, D)
                    patch_pos_embed = patch_pos_embed.reshape(1, new_patch_size * new_patch_size, -1)
                    
                    # ç»„åˆCLS tokenå’Œæ–°çš„patchä½ç½®ç¼–ç 
                    new_pos_embed = torch.cat([cls_pos_embed, patch_pos_embed], dim=1)
                    
                    # æ›´æ–°state_dict
                    state_dict['embeddings.position_embeddings'] = new_pos_embed
            
            # åŠ è½½æƒé‡
            missing_keys, unexpected_keys = self.vit.load_state_dict(state_dict, strict=False)
            if missing_keys:
                print(f"Missing keys: {missing_keys}")
            if unexpected_keys:
                print(f"Unexpected keys: {unexpected_keys}")
            print("Successfully loaded pretrained weights")
        else:
            print(f"Error: Pretrained model not found at {pretrained_model_path}")
            raise FileNotFoundError(f"Pretrained model file not found: {pretrained_model_path}")
        
        # åˆ›å»ºå¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç 
        self.weierstrass_encoding = ImprovedWeierstrassEllipticPositionalEncoding(
            d_model=self.config.hidden_size,
            num_patches_h=self.num_patches_per_side,
            num_patches_w=self.num_patches_per_side,
            use_derivative=use_derivative,
            alpha_scale=alpha_scale,
            use_layernorm=True
        )
        
        # æ›´æ–°embeddingå±‚ä»¥å¤„ç†384x384åˆ†è¾¨ç‡
        # å¯¹äº384Ã—384åˆ†è¾¨ç‡ï¼Œpatchæ•°é‡ä¸º (384/16)^2 = 576
        # ä½ç½®ç¼–ç çš„è°ƒæ•´åœ¨åŠ è½½æƒé‡æ—¶å¤„ç†
        
        # åˆ†ç±»å¤´
        self.pre_classifier = nn.Sequential(
            nn.LayerNorm(self.config.hidden_size),
            nn.Dropout(0.5)  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(self.config.hidden_size, self.config.hidden_size // 2),
            nn.GELU(),
            nn.Dropout(0.4),  # å¢åŠ dropout
            nn.Linear(self.config.hidden_size // 2, self.config.hidden_size // 4),
            nn.GELU(),
            nn.Dropout(0.3),  # å¢åŠ dropout
            nn.Linear(self.config.hidden_size // 4, num_classes)
        )
        
        # ä½ç½®ç¼–ç æ··åˆæƒé‡
        self.pos_encoding_weight = nn.Parameter(torch.tensor(0.5))
        
        # åˆå§‹åŒ–æ–°æ·»åŠ çš„å±‚
        self._init_classifier()
        
    def _init_classifier(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.8)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, pixel_values):
        B = pixel_values.shape[0]
        
        # ä½¿ç”¨ViTçš„å®Œæ•´embeddingæµç¨‹
        embeddings = self.vit.embeddings(pixel_values)
        
        # è·å–åŸå§‹ä½ç½®ç¼–ç 
        original_pos_embeddings = self.vit.embeddings.position_embeddings
        
        # è·å–å¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç 
        weierstrass_pos_encoding = self.weierstrass_encoding()  # (num_patches, D)
        
        # ç»„åˆä½ç½®ç¼–ç 
        weight = torch.sigmoid(self.pos_encoding_weight)
        cls_pos = original_pos_embeddings[:, 0:1, :]  # CLS tokenä½ç½®ç¼–ç 
        
        # æ··åˆpatchä½ç½®ç¼–ç 
        mixed_patch_pos = (weight * weierstrass_pos_encoding.unsqueeze(0) + 
                          (1 - weight) * original_pos_embeddings[:, 1:, :])
        
        # ç»„åˆCLSå’Œpatchä½ç½®ç¼–ç 
        pos_embeddings = torch.cat([cls_pos, mixed_patch_pos], dim=1)
        
        # æ›¿æ¢ä½ç½®ç¼–ç 
        # é¦–å…ˆç§»é™¤åŸå§‹çš„ä½ç½®ç¼–ç 
        patch_embeddings = embeddings[:, 1:, :]  # ç§»é™¤CLS token
        cls_token = embeddings[:, 0:1, :]  # ä¿ç•™CLS token
        
        # é‡æ–°ç»„åˆï¼Œä½¿ç”¨æ··åˆçš„ä½ç½®ç¼–ç 
        embeddings = torch.cat([cls_token, patch_embeddings + mixed_patch_pos], dim=1)
        
        # Transformerç¼–ç å™¨
        encoder_outputs = self.vit.encoder(embeddings)
        sequence_output = encoder_outputs.last_hidden_state
        
        # è·å–CLS tokençš„è¾“å‡º
        cls_output = sequence_output[:, 0]
        
        # åˆ†ç±»
        cls_output = self.pre_classifier(cls_output)
        logits = self.classifier(cls_output)
        
        return logits

class SVHNVTabDataset(Dataset):
    """SVHN VTAB-1kæ•°æ®é›†ï¼Œä½¿ç”¨å›ºå®šçš„æ•°æ®åˆ’åˆ†"""
    
    def __init__(self, train_mat_path, test_mat_path, splits_json_path, 
                 split='train', transform=None, train_ratio=0.8):
        """
        Args:
            train_mat_path: è®­ç»ƒæ•°æ®.matæ–‡ä»¶è·¯å¾„
            test_mat_path: æµ‹è¯•æ•°æ®.matæ–‡ä»¶è·¯å¾„  
            splits_json_path: VTABåˆ’åˆ†JSONæ–‡ä»¶è·¯å¾„
            split: 'train', 'val', 'test'
            transform: æ•°æ®å˜æ¢
            train_ratio: è®­ç»ƒé›†åœ¨1000ä¸ªæ ·æœ¬ä¸­çš„æ¯”ä¾‹ï¼ˆé»˜è®¤0.8ï¼Œå³800ä¸ªè®­ç»ƒï¼Œ200ä¸ªéªŒè¯ï¼‰
        """
        self.transform = transform
        self.split = split
        self.train_ratio = train_ratio
        
        # åŠ è½½VTABåˆ’åˆ†
        with open(splits_json_path, 'r') as f:
            splits_data = json.load(f)
        
        # è·å–SVHNçš„1000ä¸ªæ ·æœ¬æ–‡ä»¶å
        svhn_splits = splits_data['svhn']
        self.trainval_filenames = svhn_splits['train'] + svhn_splits['val']
        
        # å°†æ–‡ä»¶åè½¬æ¢ä¸ºç´¢å¼•ï¼ˆå»æ‰.pngåç¼€ï¼‰
        self.trainval_indices = []
        for filename in self.trainval_filenames:
            # ä»æ–‡ä»¶åæå–ç´¢å¼•ï¼Œä¾‹å¦‚ "1400.png" -> 1400
            index = int(filename.replace('.png', ''))
            self.trainval_indices.append(index)
        
        # åŠ è½½.matæ–‡ä»¶æ•°æ®
        print(f"Loading SVHN data from {train_mat_path} and {test_mat_path}")
        train_data = sio.loadmat(train_mat_path)
        test_data = sio.loadmat(test_mat_path)
        
        # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        # SVHNæ•°æ®æ ¼å¼: (32, 32, 3, N) -> è½¬æ¢ä¸º (N, 3, 32, 32)
        train_images = train_data['X'].transpose(3, 2, 0, 1)  # (32, 32, 3, N) -> (N, 3, 32, 32)
        test_images = test_data['X'].transpose(3, 2, 0, 1)   # (32, 32, 3, N) -> (N, 3, 32, 32)
        
        self.images = np.concatenate([train_images, test_images], axis=0)
        
        self.labels = np.concatenate([
            train_data['y'].flatten(),
            test_data['y'].flatten()
        ], axis=0)
        
        # SVHNæ ‡ç­¾ä»1å¼€å§‹ï¼Œéœ€è¦è½¬æ¢ä¸º0-9
        self.labels = (self.labels - 1) % 10
        
        print(f"Total images: {len(self.images)}")
        print(f"Total labels: {len(self.labels)}")
        print(f"VTAB trainval indices: {len(self.trainval_indices)}")
        
        # æ ¹æ®splité€‰æ‹©æ•°æ®
        if split in ['train', 'val']:
            # ä½¿ç”¨VTABçš„1000ä¸ªæ ·æœ¬
            num_train = int(len(self.trainval_indices) * train_ratio)
            
            if split == 'train':
                # å‰800ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ
                self.indices = self.trainval_indices[:num_train]
            else:  # val
                # å200ä¸ªæ ·æœ¬ç”¨äºéªŒè¯
                self.indices = self.trainval_indices[num_train:]
        else:  # test
            # ä½¿ç”¨æ‰€æœ‰ä¸åœ¨VTAB 1000ä¸ªæ ·æœ¬ä¸­çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†
            all_indices = set(range(len(self.images)))
            trainval_set = set(self.trainval_indices)
            self.indices = list(all_indices - trainval_set)
        
        print(f"{split} split: {len(self.indices)} samples")
        
        # éªŒè¯ç´¢å¼•çš„æœ‰æ•ˆæ€§
        max_index = max(self.indices)
        if max_index >= len(self.images):
            raise ValueError(f"Index {max_index} exceeds dataset size {len(self.images)}")
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        # è·å–å®é™…çš„æ•°æ®ç´¢å¼•
        data_idx = self.indices[idx]
        
        # è·å–å›¾åƒå’Œæ ‡ç­¾
        image = self.images[data_idx]  # (3, 32, 32)
        label = self.labels[data_idx]
        
        # ç¡®ä¿å›¾åƒæ•°æ®ç±»å‹æ­£ç¡®å¹¶è½¬æ¢ä¸ºPILå›¾åƒ
        image = image.transpose(1, 2, 0)  # (32, 32, 3)
        image = image.astype(np.uint8)  # ç¡®ä¿æ˜¯uint8ç±»å‹
        image = Image.fromarray(image)
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

def get_svhn_vtab_dataloaders(batch_size=32, num_workers=0, distributed=False, rank=0, world_size=1):
    """è·å–SVHN VTAB-1kæ•°æ®åŠ è½½å™¨ï¼Œä½¿ç”¨å›ºå®šçš„æ•°æ®åˆ’åˆ†"""
    
    # æ•°æ®è·¯å¾„
    train_mat_path = "/root/shared-nvme/VTAB/SVHN/train_32x32.mat"
    test_mat_path = "/root/shared-nvme/VTAB/SVHN/test_32x32.mat"
    splits_json_path = "/root/svhn_trainval_splits.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for path in [train_mat_path, test_mat_path, splits_json_path]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    
    # SVHNæ•°æ®é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.Resize(384),  # ç›®æ ‡åˆ†è¾¨ç‡
        transforms.RandomCrop(384, padding=32),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),  # å¢åŠ æ—‹è½¬è§’åº¦
        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.15),  # å¢å¼ºé¢œè‰²æŠ–åŠ¨
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        transforms.RandomErasing(p=0.3, scale=(0.02, 0.3), ratio=(0.3, 3.3)),  # å¢å¼ºéšæœºæ“¦é™¤
    ])
    
    transform_val = transforms.Compose([
        transforms.Resize(384),
        transforms.CenterCrop(384),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    transform_test = transforms.Compose([
        transforms.Resize(384),
        transforms.CenterCrop(384),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = SVHNVTabDataset(
        train_mat_path=train_mat_path,
        test_mat_path=test_mat_path,
        splits_json_path=splits_json_path,
        split='train',
        transform=transform_train,
        train_ratio=0.8
    )
    
    val_dataset = SVHNVTabDataset(
        train_mat_path=train_mat_path,
        test_mat_path=test_mat_path,
        splits_json_path=splits_json_path,
        split='val',
        transform=transform_val,
        train_ratio=0.8
    )
    
    test_dataset = SVHNVTabDataset(
        train_mat_path=train_mat_path,
        test_mat_path=test_mat_path,
        splits_json_path=splits_json_path,
        split='test',
        transform=transform_test,
        train_ratio=0.8
    )
    
    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Val dataset size: {len(val_dataset)}")
    print(f"Test dataset size: {len(test_dataset)}")
    
    if distributed:
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
        val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank)
        test_sampler = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank)
    else:
        train_sampler = None
        val_sampler = None
        test_sampler = None
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                            shuffle=(train_sampler is None),
                            num_workers=num_workers, 
                            pin_memory=True,
                            sampler=train_sampler,
                            drop_last=False)
    
    val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                          shuffle=False,
                          num_workers=num_workers, 
                          pin_memory=True,
                          sampler=val_sampler)
    
    test_loader = DataLoader(test_dataset, batch_size=batch_size, 
                           shuffle=False,
                           num_workers=num_workers, 
                           pin_memory=True,
                           sampler=test_sampler)
    
    return train_loader, val_loader, test_loader, train_sampler

def train_epoch(model, train_loader, criterion, optimizer, device, epoch, 
                train_sampler=None, scaler=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    if train_sampler is not None:
        train_sampler.set_epoch(epoch)
        
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
    for batch_idx, (inputs, targets) in enumerate(pbar):
        inputs, targets = inputs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        with autocast():
            outputs = model(inputs)
            loss = criterion(outputs, targets)
        
        if scaler is not None:
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
        pbar.set_postfix({
            'Loss': f'{running_loss/(batch_idx+1):.4f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })
    
    return running_loss / len(train_loader), 100. * correct / total

def evaluate(model, test_loader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in tqdm(test_loader, desc='Evaluating'):
            inputs, targets = inputs.to(device), targets.to(device)
            
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
            
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    return test_loss / len(test_loader), 100. * correct / total

def main():
    # åˆ›å»ºä¸´æ—¶ç›®å½•ä»¥è§£å†³å¤šè¿›ç¨‹é—®é¢˜
    import tempfile
    temp_dir = '/tmp/pytorch_temp'
    os.makedirs(temp_dir, exist_ok=True)
    os.environ['TMPDIR'] = temp_dir
    os.environ['TEMP'] = temp_dir
    os.environ['TMP'] = temp_dir
    
    # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        gpu = int(os.environ['LOCAL_RANK'])
        dist.init_process_group(backend='nccl', init_method='env://', 
                               world_size=world_size, rank=rank)
        torch.cuda.set_device(gpu)
        distributed = True
    else:
        distributed = False
        rank = 0
        world_size = 1
        gpu = 0
    
    device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')
    if rank == 0:
        print(f"Using device: {device}")
        print(f"Distributed training: {distributed}")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42 + rank)
    
    # é’ˆå¯¹VTAB-1kä¼˜åŒ–çš„è¶…å‚æ•° - å¹³è¡¡å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–
    batch_size = 16  # è¾ƒå°çš„batch sizeé€‚åˆå°æ•°æ®é›†
    learning_rate = 2e-4  # é€‚ä¸­çš„å­¦ä¹ ç‡
    num_epochs = 100  # VTAB-1kè®¾ç½®
    warmup_epochs = 10  # é€‚ä¸­çš„warmupè½®æ•°
    weight_decay = 0.02  # é€‚ä¸­çš„æƒé‡è¡°å‡
    num_workers = 0  # ç¦ç”¨å¤šè¿›ç¨‹ä»¥é¿å…ä¸´æ—¶ç›®å½•é—®é¢˜
    
    # æ¨¡å‹å‚æ•°
    pretrained_model_path = "/root/shared-nvme/vit_l16_in21k.pth"
    num_classes = 10  # SVHNæœ‰10ä¸ªç±»åˆ«
    image_size = 384
    
    if rank == 0:
        print("Loading ViT-L/16 model from local pretrained weights...")
    
    # åˆ›å»ºæ¨¡å‹
    model = WeierstrassViTL16(
        pretrained_model_path=pretrained_model_path,
        num_classes=num_classes,
        image_size=image_size,
        use_derivative=True,
        alpha_scale=0.03
    )
    
    model = model.to(device)
    
    if distributed:
        model = DDP(model, device_ids=[gpu], find_unused_parameters=True)
    
    # æ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader, train_sampler = get_svhn_vtab_dataloaders(
        batch_size=batch_size,
        num_workers=num_workers,
        distributed=distributed,
        rank=rank,
        world_size=world_size
    )
    
    # æŸå¤±å‡½æ•° - é€‚ä¸­çš„æ ‡ç­¾å¹³æ»‘
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    # ä¼˜åŒ–å™¨ - ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
    if distributed:
        vit_params = []
        weierstrass_params = []
        classifier_params = []
        
        for name, param in model.module.named_parameters():
            if 'weierstrass_encoding' in name:
                weierstrass_params.append(param)
            elif 'classifier' in name or 'pre_classifier' in name or 'pos_encoding_weight' in name:
                classifier_params.append(param)
            else:
                vit_params.append(param)
    else:
        vit_params = []
        weierstrass_params = []
        classifier_params = []
        
        for name, param in model.named_parameters():
            if 'weierstrass_encoding' in name:
                weierstrass_params.append(param)
            elif 'classifier' in name or 'pre_classifier' in name or 'pos_encoding_weight' in name:
                classifier_params.append(param)
            else:
                vit_params.append(param)
    
    optimizer = optim.AdamW([
        {'params': vit_params, 'lr': learning_rate * 0.1},  # é¢„è®­ç»ƒå±‚è¾ƒå°å­¦ä¹ ç‡
        {'params': weierstrass_params, 'lr': learning_rate},  # å¨å°”æ–¯ç‰¹æ‹‰æ–¯ç¼–ç æ­£å¸¸å­¦ä¹ ç‡
        {'params': classifier_params, 'lr': learning_rate * 1.5}  # åˆ†ç±»å™¨è¾ƒå¤§å­¦ä¹ ç‡
    ], weight_decay=weight_decay, betas=(0.9, 0.999))
    
    # æ¢¯åº¦ç¼©æ”¾å™¨
    scaler = GradScaler()
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç«
    def cosine_lr_lambda(epoch):
        if epoch < warmup_epochs:
            return epoch / warmup_epochs
        else:
            return 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, cosine_lr_lambda)
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
    checkpoint_dir = "/root/shared-nvme/checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    best_val_acc = 0.0
    results = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    patience = 20  # å¢åŠ patienceé˜²æ­¢è¿‡æ—©åœæ­¢
    patience_counter = 0
    
    if rank == 0:
        print("\n=== å¼€å§‹SVHN VTAB-1kå¾®è°ƒå®éªŒ ===")
        print(f"ç›®æ ‡ï¼šè¶…è¿‡åŸºçº¿å‡†ç¡®ç‡ 80.9%")
        print(f"è®­ç»ƒæ ·æœ¬æ•°ï¼š800 (VTAB-1kæ ‡å‡†)")
        print(f"éªŒè¯æ ·æœ¬æ•°ï¼š200 (VTAB-1kæ ‡å‡†)")
        print(f"æµ‹è¯•æ ·æœ¬æ•°ï¼š26,032 (VTAB-1kæ ‡å‡†)")
        print(f"å›¾åƒåˆ†è¾¨ç‡ï¼š{image_size}x{image_size}")
        print(f"æ¨¡å‹ï¼šViT-L/16 + å¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç ")
    
    for epoch in range(1, num_epochs + 1):
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch, 
            train_sampler, scaler
        )
        
        # éªŒè¯
        val_loss, val_acc = evaluate(model, val_loader, criterion, device)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•ç»“æœ
        results['train_loss'].append(train_loss)
        results['train_acc'].append(train_acc)
        results['val_loss'].append(val_loss)
        results['val_acc'].append(val_acc)
        
        if rank == 0:
            print(f"\nEpoch {epoch}/{num_epochs}:")
            print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†ï¼‰
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                
                model_state = model.module.state_dict() if distributed else model.state_dict()
                best_model_path = os.path.join(checkpoint_dir, 'weierstrass_vit_l16_svhn_best.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model_state,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'best_val_acc': best_val_acc,
                    'results': results
                }, best_model_path)
                
                print(f"ğŸ† æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
            else:
                patience_counter += 1
                if patience_counter >= patience and epoch > 30:
                    print(f"Early stopping at epoch {epoch}")
                    break
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 20 == 0:
                model_state = model.module.state_dict() if distributed else model.state_dict()
                checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model_state,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'results': results
                }, checkpoint_path)
    
    # è®­ç»ƒå®Œæˆåï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    if rank == 0:
        print(f"\n=== è®­ç»ƒå®Œæˆï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯• ===")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = os.path.join(checkpoint_dir, 'weierstrass_vit_l16_svhn_best.pth')
        checkpoint = torch.load(best_model_path)
        if distributed:
            model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"åŠ è½½ç¬¬ {checkpoint['epoch']} è½®çš„æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {checkpoint['best_val_acc']:.2f}%")
    
    # æœ€ç»ˆæµ‹è¯•è¯„ä¼°
    test_loss, test_acc = evaluate(model, test_loader, criterion, device)
    
    if rank == 0:
        print(f"\n=== æœ€ç»ˆæµ‹è¯•ç»“æœ ===")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
        print(f"åŸºçº¿å‡†ç¡®ç‡: 80.9%")
        
        if test_acc > 80.9:
            improvement = test_acc - 80.9
            print(f"ğŸ‰ æˆåŠŸè¶…è¿‡åŸºçº¿! æå‡äº† {improvement:.2f} ä¸ªç™¾åˆ†ç‚¹")
        else:
            print(f"æœªè¾¾åˆ°ç›®æ ‡ï¼Œè¿˜éœ€è¦ {80.9 - test_acc:.2f} ä¸ªç™¾åˆ†ç‚¹")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        results['final_test_acc'] = test_acc
        results['best_val_acc'] = best_val_acc
        results['baseline_accuracy'] = 80.9
        results['improvement'] = test_acc - 80.9
        
        results_path = os.path.join(checkpoint_dir, 'svhn_vtab_results.json')
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=4)
        
        # è¾“å‡ºå­¦ä¹ åˆ°çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‚æ•°
        if distributed:
            alpha = F.softplus(model.module.weierstrass_encoding.alpha_learn).item()
            beta = F.softplus(model.module.weierstrass_encoding.beta_learn).item()
            gamma = F.softplus(model.module.weierstrass_encoding.gamma_learn).item()
            weight = torch.sigmoid(model.module.pos_encoding_weight).item()
        else:
            alpha = F.softplus(model.weierstrass_encoding.alpha_learn).item()
            beta = F.softplus(model.weierstrass_encoding.beta_learn).item()
            gamma = F.softplus(model.weierstrass_encoding.gamma_learn).item()
            weight = torch.sigmoid(model.pos_encoding_weight).item()
        
        print(f"\nå­¦ä¹ åˆ°çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‚æ•°:")
        print(f"Ï‰'2 (alpha): {alpha:.6f}")
        print(f"Î² (beta): {beta:.6f}")
        print(f"Î³ (gamma): {gamma:.6f}")
        print(f"ä½ç½®ç¼–ç æ··åˆæƒé‡: {weight:.6f}")
        
        print(f"\n=== å®éªŒæ€»ç»“ ===")
        print(f"VTAB-1kæ ‡å‡†é…ç½®:")
        print(f"- è®­ç»ƒæ ·æœ¬: 800")
        print(f"- éªŒè¯æ ·æœ¬: 200") 
        print(f"- æµ‹è¯•æ ·æœ¬: 26,032")
        print(f"- æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        print(f"- æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
        print(f"- åŸºçº¿å‡†ç¡®ç‡: 80.9%")
        print(f"- æå‡å¹…åº¦: {test_acc - 80.9:.2f} ä¸ªç™¾åˆ†ç‚¹")
    
    if distributed:
        dist.destroy_process_group()

if __name__ == "__main__":
    main()