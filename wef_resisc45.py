import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import numpy as np
import math
from scipy.special import gamma
import os
import json
from tqdm import tqdm
import warnings
import time
import random
from transformers import ViTModel, ViTConfig
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import autocast, GradScaler
from PIL import Image
import glob
warnings.filterwarnings('ignore')

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥è§£å†³å¤šè¿›ç¨‹é—®é¢˜
os.environ['TMPDIR'] = '/tmp'
os.environ['TEMP'] = '/tmp'
os.environ['TMP'] = '/tmp'

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿å®éªŒå¯é‡å¤æ€§"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class ImprovedWeierstrassEllipticPositionalEncoding(nn.Module):
    """æ”¹è¿›çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯æ¤­åœ†å‡½æ•°ä½ç½®ç¼–ç  - é’ˆå¯¹ViT-L/16ä¼˜åŒ–"""
    
    def __init__(self, d_model, num_patches_h, num_patches_w, 
                 use_derivative=True, alpha_scale=0.02, use_layernorm=True):
        super().__init__()
        self.d_model = d_model
        self.num_patches_h = num_patches_h
        self.num_patches_w = num_patches_w
        self.use_derivative = use_derivative
        self.alpha_scale = alpha_scale
        self.use_layernorm = use_layernorm
        
        # å¨å°”æ–¯ç‰¹æ‹‰æ–¯æ¤­åœ†å‡½æ•°çš„ç²¾ç¡®åŠå‘¨æœŸ
        self.omega1 = gamma(0.25)**2 / (2 * np.sqrt(2 * np.pi))
        
        # å¯å­¦ä¹ å‚æ•° - é’ˆå¯¹é¥æ„Ÿå›¾åƒè°ƒæ•´åˆå§‹å€¼
        self.alpha_learn = nn.Parameter(torch.tensor(0.6))
        self.beta_learn = nn.Parameter(torch.tensor(0.15))
        self.gamma_learn = nn.Parameter(torch.tensor(0.25))
        
        # å¤šå±‚æŠ•å½±ç½‘ç»œ - å¢åŠ å®¹é‡ä»¥é€‚åº”é¥æ„Ÿå›¾åƒå¤æ‚æ€§
        input_dim = 4 if use_derivative else 2
        self.projection = nn.Sequential(
            nn.Linear(input_dim, d_model // 2),
            nn.LayerNorm(d_model // 2) if use_layernorm else nn.Identity(),
            nn.GELU(),
            nn.Dropout(0.15),
            nn.Linear(d_model // 2, d_model),
            nn.LayerNorm(d_model) if use_layernorm else nn.Identity(),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model, d_model)
        )
        
        # å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»
        self.scale_factor = nn.Parameter(torch.tensor(alpha_scale))
        self.offset = nn.Parameter(torch.zeros(1, 1, d_model))
        
        # é¢‘ç‡è°ƒåˆ¶å‚æ•°
        freq_dim = 4 if use_derivative else 2
        self.freq_modulation = nn.Parameter(torch.ones(freq_dim))
        
        # åˆå§‹åŒ–
        self._init_parameters()
        
    def _init_parameters(self):
        """Xavieråˆå§‹åŒ–"""
        with torch.no_grad():
            for m in self.projection.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight, gain=0.9)
                    nn.init.zeros_(m.bias)
    
    def stable_weierstrass_p(self, z):
        """æ•°å€¼ç¨³å®šçš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯æ¤­åœ†å‡½æ•° - é’ˆå¯¹é¥æ„Ÿå›¾åƒä¼˜åŒ–"""
        omega2_prime = 0.12 + F.softplus(self.alpha_learn)
        beta = 0.015 + F.softplus(self.beta_learn)
        gamma_param = 0.15 + F.softplus(self.gamma_learn)
        
        z_real = z.real
        z_imag = z.imag
        z_mod = torch.sqrt(z_real**2 + z_imag**2)
        z_mod_safe = torch.clamp(z_mod, min=1e-6)
        
        # ä¸»é¡¹
        main_term = 1.0 / (z_mod_safe**2 + beta)
        
        # å‘¨æœŸä¿®æ­£é¡¹ - å¢åŠ å¤æ‚åº¦ä»¥æ•è·é¥æ„Ÿå›¾åƒçš„ç©ºé—´å…³ç³»
        u = z_real / self.omega1
        v = z_imag / omega2_prime
        
        correction = torch.zeros_like(z_real)
        for k in range(1, 5):  # å¢åŠ é¡¹æ•°
            freq_factor = k * math.pi
            amp_k = gamma_param / (k**1.8)  # è°ƒæ•´è¡°å‡é€Ÿåº¦
            
            correction += amp_k * (
                torch.cos(freq_factor * u) * torch.exp(-freq_factor * torch.abs(v) * 0.8) +
                torch.sin(freq_factor * v) * torch.exp(-freq_factor * torch.abs(u) * 0.8)
            )
        
        p_z_real = main_term * torch.cos(torch.atan2(z_imag, z_real)) + correction
        p_z_imag = main_term * torch.sin(torch.atan2(z_imag, z_real)) + correction * 0.75
        
        return torch.complex(p_z_real, p_z_imag)
    
    def stable_weierstrass_p_derivative(self, z):
        """å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‡½æ•°å¯¼æ•°"""
        omega2_prime = 0.12 + F.softplus(self.alpha_learn)
        beta = 0.015 + F.softplus(self.beta_learn)
        gamma_param = 0.15 + F.softplus(self.gamma_learn)
        
        z_mod = torch.sqrt(z.real**2 + z.imag**2)
        z_mod_safe = torch.clamp(z_mod, min=1e-6)
        
        main_deriv = -2.0 / (z_mod_safe**3 + beta)
        
        u = z.real / self.omega1
        v = z.imag / omega2_prime
        
        deriv_correction = torch.zeros_like(z.real)
        for k in range(1, 4):
            freq_factor = k * math.pi
            amp_k = gamma_param / (k**1.8)
            
            deriv_correction += amp_k * k * (
                -torch.sin(freq_factor * u) * torch.exp(-freq_factor * torch.abs(v) * 0.8) +
                torch.cos(freq_factor * v) * torch.exp(-freq_factor * torch.abs(u) * 0.8)
            )
        
        p_deriv_real = main_deriv * torch.cos(torch.atan2(z.imag, z.real)) + deriv_correction
        p_deriv_imag = main_deriv * torch.sin(torch.atan2(z.imag, z.real)) + deriv_correction * 0.6
        
        return torch.complex(p_deriv_real, p_deriv_imag)
    
    def forward(self, num_patches_h=None, num_patches_w=None):
        """ç”Ÿæˆä½ç½®ç¼–ç """
        if num_patches_h is None:
            num_patches_h = self.num_patches_h
        if num_patches_w is None:
            num_patches_w = self.num_patches_w
            
        device = self.alpha_learn.device
        
        # åˆ›å»ºç½‘æ ¼åæ ‡
        row_idx = torch.arange(num_patches_h, dtype=torch.float32, device=device)
        col_idx = torch.arange(num_patches_w, dtype=torch.float32, device=device)
        
        # æ”¹è¿›çš„å½’ä¸€åŒ– - é’ˆå¯¹é¥æ„Ÿå›¾åƒçš„ç©ºé—´åˆ†å¸ƒ
        u = (col_idx + 0.5) / (num_patches_w + 1e-8)
        v = (row_idx + 0.5) / (num_patches_h + 1e-8)
        
        u_grid, v_grid = torch.meshgrid(u, v, indexing='xy')
        u_grid = u_grid.T.reshape(-1)
        v_grid = v_grid.T.reshape(-1)
        
        omega2_prime = 0.12 + F.softplus(self.alpha_learn)
        
        # æ˜ å°„åˆ°å¤å¹³é¢ï¼Œå¢åŠ éçº¿æ€§å˜æ¢ä»¥é€‚åº”é¥æ„Ÿå›¾åƒ
        z_real = u_grid * self.omega1 + 0.025 * torch.sin(3.5 * math.pi * u_grid)
        z_imag = v_grid * omega2_prime + 0.025 * torch.cos(3.5 * math.pi * v_grid)
        z = torch.complex(z_real, z_imag)
        
        # è®¡ç®—å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‡½æ•°å€¼
        p_z = self.stable_weierstrass_p(z)
        
        # åˆ†ç¦»å®éƒ¨å’Œè™šéƒ¨
        p_real = p_z.real * self.freq_modulation[0]
        p_imag = p_z.imag * self.freq_modulation[1]
        
        # æ”¹è¿›çš„æ¿€æ´»å‡½æ•°
        pe_real = torch.tanh(self.scale_factor * p_real)
        pe_imag = torch.tanh(self.scale_factor * p_imag * 0.85)
        
        if self.use_derivative:
            p_prime = self.stable_weierstrass_p_derivative(z)
            p_prime_real = torch.tanh(self.scale_factor * p_prime.real * self.freq_modulation[2])
            p_prime_imag = torch.tanh(self.scale_factor * p_prime.imag * self.freq_modulation[3])
            
            features = torch.stack([pe_real, pe_imag, p_prime_real, p_prime_imag], dim=-1)
        else:
            features = torch.stack([pe_real, pe_imag], dim=-1)
        
        # æŠ•å½±åˆ°d_modelç»´
        pos_encoding = self.projection(features)
        pos_encoding = pos_encoding + self.offset
        
        return pos_encoding.squeeze(0) if pos_encoding.dim() == 3 else pos_encoding

class WeierstrassViTL16(nn.Module):
    """åŸºäºæœ¬åœ°ViT-L/16é¢„è®­ç»ƒæƒé‡çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç æ¨¡å‹"""
    
    def __init__(self, pretrained_model_path="/root/shared-nvme/vit_l16_in21k.pth", 
                 num_classes=45, image_size=384, use_derivative=True, alpha_scale=0.02):
        super().__init__()
        
        # åˆ›å»ºViT-L/16é…ç½®ï¼Œç¡®ä¿å‚æ•°åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
        self.config = ViTConfig(
            hidden_size=1024,          # ViT-L/16æ ‡å‡†é…ç½®
            num_hidden_layers=24,      # ViT-L/16æ ‡å‡†é…ç½®
            num_attention_heads=16,    # ViT-L/16æ ‡å‡†é…ç½®
            intermediate_size=4096,    # ViT-L/16æ ‡å‡†é…ç½® (é‡è¦ï¼)
            hidden_dropout_prob=0.0,
            attention_probs_dropout_prob=0.0,
            initializer_range=0.02,
            layer_norm_eps=1e-12,
            image_size=image_size,     # 384Ã—384
            patch_size=16,
            num_channels=3,
            qkv_bias=True,
            encoder_stride=16,
        )
        
        # åˆ›å»ºViTæ¨¡å‹ç»“æ„ï¼ˆä¸åŒ…å«poolingå±‚ä»¥é¿å…å°ºå¯¸é—®é¢˜ï¼‰
        self.vit = ViTModel(self.config, add_pooling_layer=False)
        
        # è®¡ç®—patchæ•°é‡
        self.image_size = image_size
        self.patch_size = self.config.patch_size
        self.num_patches_per_side = image_size // self.patch_size  # 24 for 384Ã—384
        self.num_patches = self.num_patches_per_side ** 2  # 576
        
        print(f"Model configuration:")
        print(f"- Image size: {image_size}x{image_size}")
        print(f"- Patch size: {self.patch_size}x{self.patch_size}")
        print(f"- Number of patches per side: {self.num_patches_per_side}")
        print(f"- Total number of patches: {self.num_patches}")
        print(f"- Hidden size: {self.config.hidden_size}")
        print(f"- Intermediate size: {self.config.intermediate_size}")
        
        # åŠ è½½æœ¬åœ°é¢„è®­ç»ƒæƒé‡å¹¶å¤„ç†å°ºå¯¸ä¸åŒ¹é…
        if pretrained_model_path is not None and isinstance(pretrained_model_path, str) and os.path.exists(pretrained_model_path):
            print(f"Loading pretrained ViT-L/16 weights from: {pretrained_model_path}")
            state_dict = torch.load(pretrained_model_path, map_location="cpu")
            # å¤„ç†å¯èƒ½çš„moduleå‰ç¼€
            if any(k.startswith('module.') for k in state_dict.keys()):
                state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
            # å¤„ç†ä½ç½®ç¼–ç å°ºå¯¸ä¸åŒ¹é…é—®é¢˜
            if 'embeddings.position_embeddings' in state_dict:
                old_pos_embed = state_dict['embeddings.position_embeddings']
                print(f"Original position embedding shape: {old_pos_embed.shape}")
                old_seq_len = old_pos_embed.shape[1]
                new_seq_len = self.num_patches + 1
                if old_seq_len != new_seq_len:
                    print(f"Resizing position embeddings from {old_seq_len} to {new_seq_len}")
                    cls_pos_embed = old_pos_embed[:, 0:1, :]
                    patch_pos_embed = old_pos_embed[:, 1:, :]
                    old_grid_size = int(np.sqrt(old_seq_len - 1))
                    new_grid_size = int(np.sqrt(new_seq_len - 1))
                    print(f"Interpolating from {old_grid_size}x{old_grid_size} to {new_grid_size}x{new_grid_size}")
                    patch_pos_embed = patch_pos_embed.reshape(1, old_grid_size, old_grid_size, -1)
                    patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)
                    patch_pos_embed = F.interpolate(
                        patch_pos_embed, 
                        size=(new_grid_size, new_grid_size), 
                        mode='bilinear', 
                        align_corners=False
                    )
                    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1)
                    patch_pos_embed = patch_pos_embed.reshape(1, new_grid_size * new_grid_size, -1)
                    new_pos_embed = torch.cat([cls_pos_embed, patch_pos_embed], dim=1)
                    state_dict['embeddings.position_embeddings'] = new_pos_embed
                    print(f"New position embedding shape: {new_pos_embed.shape}")
            keys_to_remove = [k for k in state_dict.keys() if k.startswith('pooler.')]
            for key in keys_to_remove:
                del state_dict[key]
                print(f"Removed pooler layer: {key}")
            missing_keys, unexpected_keys = self.vit.load_state_dict(state_dict, strict=False)
            print(f"Missing keys: {len(missing_keys)}")
            print(f"Unexpected keys: {len(unexpected_keys)}")
            if missing_keys:
                print(f"Missing: {missing_keys}")
            if unexpected_keys:
                print(f"Unexpected: {unexpected_keys}")
            print("Successfully loaded pretrained weights")
        else:
            print("No pretrained weights loaded for ViT-L/16 (pretrained_model_path is None or file does not exist)")
        
        # åˆ›å»ºå¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç 
        self.weierstrass_encoding = ImprovedWeierstrassEllipticPositionalEncoding(
            d_model=self.config.hidden_size,
            num_patches_h=self.num_patches_per_side,
            num_patches_w=self.num_patches_per_side,
            use_derivative=use_derivative,
            alpha_scale=alpha_scale,
            use_layernorm=True
        )
        
        # åˆ†ç±»å¤´ - é’ˆå¯¹RESISC45ä¼˜åŒ–
        self.pre_classifier = nn.Sequential(
            nn.LayerNorm(self.config.hidden_size),
            nn.Dropout(0.6)  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(self.config.hidden_size, self.config.hidden_size // 2),
            nn.GELU(),
            nn.Dropout(0.5),
            nn.Linear(self.config.hidden_size // 2, self.config.hidden_size // 4),
            nn.GELU(),
            nn.Dropout(0.4),
            nn.Linear(self.config.hidden_size // 4, num_classes)
        )
        
        # ä½ç½®ç¼–ç æ··åˆæƒé‡
        self.pos_encoding_weight = nn.Parameter(torch.tensor(0.6))  # å¢åŠ å¨å°”æ–¯ç‰¹æ‹‰æ–¯ç¼–ç æƒé‡
        
        # åˆå§‹åŒ–æ–°æ·»åŠ çš„å±‚
        self._init_classifier()
        
        print("Model initialized successfully!")
        
    def _init_classifier(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.9)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, pixel_values):
        B = pixel_values.shape[0]
        
        # ä½¿ç”¨ViTçš„å®Œæ•´embeddingæµç¨‹
        embeddings = self.vit.embeddings(pixel_values)
        
        # è·å–åŸå§‹ä½ç½®ç¼–ç 
        original_pos_embeddings = self.vit.embeddings.position_embeddings
        
        # è·å–å¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç 
        weierstrass_pos_encoding = self.weierstrass_encoding()  # (num_patches, D)
        
        # ç»„åˆä½ç½®ç¼–ç 
        weight = torch.sigmoid(self.pos_encoding_weight)
        cls_pos = original_pos_embeddings[:, 0:1, :]  # CLS tokenä½ç½®ç¼–ç 
        
        # æ··åˆpatchä½ç½®ç¼–ç 
        mixed_patch_pos = (weight * weierstrass_pos_encoding.unsqueeze(0) + 
                          (1 - weight) * original_pos_embeddings[:, 1:, :])
        
        # ç»„åˆCLSå’Œpatchä½ç½®ç¼–ç 
        pos_embeddings = torch.cat([cls_pos, mixed_patch_pos], dim=1)
        
        # æ›¿æ¢ä½ç½®ç¼–ç 
        patch_embeddings = embeddings[:, 1:, :]  # ç§»é™¤CLS token
        cls_token = embeddings[:, 0:1, :]  # ä¿ç•™CLS token
        
        # é‡æ–°ç»„åˆï¼Œä½¿ç”¨æ··åˆçš„ä½ç½®ç¼–ç 
        embeddings = torch.cat([cls_token, patch_embeddings + mixed_patch_pos], dim=1)
        
        # Transformerç¼–ç å™¨
        encoder_outputs = self.vit.encoder(embeddings)
        sequence_output = encoder_outputs.last_hidden_state
        
        # è·å–CLS tokençš„è¾“å‡º
        cls_output = sequence_output[:, 0]
        
        # åˆ†ç±»
        cls_output = self.pre_classifier(cls_output)
        logits = self.classifier(cls_output)
        
        return logits

class RESISC45VTabDataset(Dataset):
    """RESISC45 VTAB-1kæ•°æ®é›†ï¼Œä½¿ç”¨å›ºå®šçš„æ•°æ®åˆ’åˆ†"""
    
    def __init__(self, data_root, splits_json_path, 
                 split='train', transform=None, train_ratio=0.8, test_files_json=None):
        self.data_root = data_root
        self.transform = transform
        self.split = split
        self.train_ratio = train_ratio
        self.class_names = [
            'airplane', 'airport', 'baseball_diamond', 'basketball_court', 'beach',
            'bridge', 'chaparral', 'church', 'circular_farmland', 'cloud',
            'commercial_area', 'dense_residential', 'desert', 'forest', 'freeway',
            'golf_course', 'ground_track_field', 'harbor', 'industrial_area', 'intersection',
            'island', 'lake', 'meadow', 'medium_residential', 'mobile_home_park',
            'mountain', 'overpass', 'palace', 'parking_lot', 'railway',
            'railway_station', 'rectangular_farmland', 'river', 'roundabout', 'runway',
            'sea_ice', 'ship', 'snowberg', 'sparse_residential', 'stadium',
            'storage_tank', 'tennis_court', 'terrace', 'thermal_power_station', 'wetland'
        ]
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.class_names)}
        with open(splits_json_path, 'r', encoding='utf-8-sig') as f:
            splits_data = json.load(f)
        resisc45_splits = splits_data['resisc45']
        self.trainval_filenames = resisc45_splits['train'] + resisc45_splits['val']
        print(f"VTAB trainval filenames: {len(self.trainval_filenames)}")
        self.images = []
        self.labels = []
        if split in ['train', 'val']:
            num_train = int(len(self.trainval_filenames) * train_ratio)
            if split == 'train':
                filenames = self.trainval_filenames[:num_train]
            else:
                filenames = self.trainval_filenames[num_train:]
            used_set = set()
            for filename in filenames:
                class_name = '_'.join(filename.split('_')[:-1])
                if class_name in self.class_to_idx:
                    image_path = os.path.join(data_root, class_name, filename)
                    if os.path.exists(image_path):
                        self.images.append(image_path)
                        self.labels.append(self.class_to_idx[class_name])
                        used_set.add(filename)
                    else:
                        print(f"Warning: Image not found: {image_path}")
            # è‡ªåŠ¨è¡¥è¶³ç¼ºå¤±å›¾ç‰‡
            missing_count = (num_train if split == 'train' else len(filenames)) - len(self.images)
            if missing_count > 0:
                print(f"{split} split: è¡¥è¶³ç¼ºå¤±å›¾ç‰‡ {missing_count} å¼ ")
                for class_name in self.class_names:
                    class_dir = os.path.join(data_root, class_name)
                    if os.path.exists(class_dir):
                        all_imgs = [f for f in os.listdir(class_dir) if f.endswith((".jpg", ".png", ".jpeg"))]
                        candidates = [f for f in all_imgs if f not in used_set]
                        random.shuffle(candidates)
                        for f in candidates:
                            if missing_count <= 0:
                                break
                            image_path = os.path.join(class_dir, f)
                            self.images.append(image_path)
                            self.labels.append(self.class_to_idx[class_name])
                            used_set.add(f)
                            missing_count -= 1
                        if missing_count <= 0:
                            break
        else:  # test
            # æµ‹è¯•é›†ä¸¥æ ¼é™å®šä¸ºtest_files_jsonæŒ‡å®šçš„å›¾ç‰‡
            assert test_files_json is not None, "test_files_json must be provided for test split!"
            with open(test_files_json, 'r', encoding='utf-8') as f:
                test_files = json.load(f)
            for rel_path in test_files:
                class_name = rel_path.split('/')[0]
                if class_name in self.class_to_idx:
                    image_path = os.path.join(data_root, rel_path)
                    if os.path.exists(image_path):
                        self.images.append(image_path)
                        self.labels.append(self.class_to_idx[class_name])
                    else:
                        print(f"Warning: Test image not found: {image_path}")
            print(f"test split: {len(self.images)} samples (ä¸¥æ ¼æŒ‰test_files_json)")
        print(f"{split} split: {len(self.images)} samples")
        print(f"Number of classes: {len(set(self.labels))}")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        # è·å–å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        image_path = self.images[idx]
        label = self.labels[idx]
        
        # åŠ è½½å›¾åƒ
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            # åˆ›å»ºä¸€ä¸ªé»‘è‰²å›¾åƒä½œä¸ºå¤‡ç”¨
            image = Image.new('RGB', (384, 384), (0, 0, 0))  # ä½¿ç”¨384Ã—384
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

def get_resisc45_vtab_dataloaders(batch_size=16, num_workers=4, distributed=False, rank=0, world_size=1):
    """è·å–RESISC45 VTAB-1kæ•°æ®åŠ è½½å™¨ï¼Œä½¿ç”¨å›ºå®šçš„æ•°æ®åˆ’åˆ†"""
    
    # æ•°æ®æ ¹ç›®å½•ï¼ˆæ— train/testå­æ–‡ä»¶å¤¹ï¼‰
    data_root = "/root/shared-nvme/VTAB/RESISC45/NWPU-RESISC45"
    splits_json_path = "/root/resisc45_trainval_splits.json"
    test_files_json = "/root/resisc45_test_files.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_root):
        raise FileNotFoundError(f"æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {data_root}")
    if not os.path.exists(splits_json_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {splits_json_path}")
    if not os.path.exists(test_files_json):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {test_files_json}")
    
    # RESISC45æ•°æ®é¢„å¤„ç† - é’ˆå¯¹384Ã—384åˆ†è¾¨ç‡ä¼˜åŒ–
    transform_train = transforms.Compose([
        transforms.Resize(416),  # ç¨å¤§ä¸€äº›çš„resizeä»¥ä¿ç•™æ›´å¤šç»†èŠ‚
        transforms.RandomCrop(384, padding=32),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.3),  # é¥æ„Ÿå›¾åƒå¯ä»¥å‚ç›´ç¿»è½¬
        transforms.RandomRotation(degrees=30),  # æ›´å¤§çš„æ—‹è½¬è§’åº¦ï¼Œå› ä¸ºé¥æ„Ÿå›¾åƒæ–¹å‘ä¸å›ºå®š
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        transforms.RandomErasing(p=0.4, scale=(0.02, 0.25), ratio=(0.3, 3.3)),
    ])
    
    transform_val = transforms.Compose([
        transforms.Resize(416),
        transforms.CenterCrop(384),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    transform_test = transforms.Compose([
        transforms.Resize(416),
        transforms.CenterCrop(384),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = RESISC45VTabDataset(
        data_root=data_root,
        splits_json_path=splits_json_path,
        split='train',
        transform=transform_train,
        train_ratio=1.0  # ä½¿ç”¨å…¨éƒ¨1000ä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒé›†
    )
    
    # ä¸å†å•ç‹¬åˆ’åˆ†éªŒè¯é›†
    val_dataset = None
    
    test_dataset = RESISC45VTabDataset(
        data_root=data_root,
        splits_json_path=splits_json_path,
        split='test',
        transform=transform_test,
        train_ratio=1.0,
        test_files_json=test_files_json
    )
    
    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Test dataset size: {len(test_dataset)}")
    
    if distributed:
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
        test_sampler = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank)
    else:
        train_sampler = None
        test_sampler = None
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                            shuffle=(train_sampler is None),
                            num_workers=num_workers, 
                            pin_memory=True,
                            sampler=train_sampler,
                            drop_last=False)
    
    test_loader = DataLoader(test_dataset, batch_size=batch_size, 
                           shuffle=False,
                           num_workers=num_workers, 
                           pin_memory=True,
                           sampler=test_sampler)
    
    return train_loader, test_loader, train_sampler

def train_epoch(model, train_loader, criterion, optimizer, device, epoch, 
                train_sampler=None, scaler=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    if train_sampler is not None:
        train_sampler.set_epoch(epoch)
        
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
    for batch_idx, (inputs, targets) in enumerate(pbar):
        inputs, targets = inputs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        with autocast():
            outputs = model(inputs)
            loss = criterion(outputs, targets)
        
        if scaler is not None:
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
        pbar.set_postfix({
            'Loss': f'{running_loss/(batch_idx+1):.4f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })
    
    return running_loss / len(train_loader), 100. * correct / total

def evaluate(model, test_loader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in tqdm(test_loader, desc='Evaluating'):
            inputs, targets = inputs.to(device), targets.to(device)
            
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
            
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    return test_loss / len(test_loader), 100. * correct / total

def main():
    # åˆ›å»ºä¸´æ—¶ç›®å½•ä»¥è§£å†³å¤šè¿›ç¨‹é—®é¢˜
    import tempfile
    temp_dir = '/tmp/pytorch_temp'
    os.makedirs(temp_dir, exist_ok=True)
    os.environ['TMPDIR'] = temp_dir
    os.environ['TEMP'] = temp_dir
    os.environ['TMP'] = temp_dir
    
    # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        gpu = int(os.environ['LOCAL_RANK'])
        dist.init_process_group(backend='nccl', init_method='env://', 
                               world_size=world_size, rank=rank)
        torch.cuda.set_device(gpu)
        distributed = True
    else:
        distributed = False
        rank = 0
        world_size = 1
        gpu = 0
    
    device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')
    if rank == 0:
        print(f"Using device: {device}")
        print(f"Distributed training: {distributed}")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42 + rank)
    
    # é’ˆå¯¹RESISC45å’ŒVTAB-1kä¼˜åŒ–çš„è¶…å‚æ•°
    batch_size = 12  # é€‚ä¸­çš„batch sizeï¼Œè€ƒè™‘åˆ°4å¡24GBæ˜¾å­˜
    learning_rate = 1.5e-4  # è¾ƒå°çš„å­¦ä¹ ç‡é€‚åˆé¥æ„Ÿå›¾åƒ
    num_epochs = 120  # å¢åŠ è®­ç»ƒè½®æ•°
    warmup_epochs = 15  # æ›´é•¿çš„warmup
    weight_decay = 0.08  # é€‚ä¸­çš„æƒé‡è¡°å‡
    num_workers = 4  # å‡å°‘workeræ•°é‡é¿å…å†…å­˜é—®é¢˜
    
    # æ¨¡å‹å‚æ•°
    pretrained_model_path = "/root/shared-nvme/vit_l16_in21k.pth"  # æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹
    num_classes = 45  # RESISC45æœ‰45ä¸ªç±»åˆ«
    image_size = 384  # ç¡®ä¿ä½¿ç”¨384Ã—384åˆ†è¾¨ç‡
    
    if rank == 0:
        print("Loading ViT-L/16 model from local checkpoint...")
        print(f"Target image size: {image_size}x{image_size}")
    
    # åˆ›å»ºæ¨¡å‹
    model = WeierstrassViTL16(
        pretrained_model_path=pretrained_model_path,
        num_classes=num_classes,
        image_size=image_size,
        use_derivative=True,
        alpha_scale=0.02
    )
    
    model = model.to(device)
    
    if distributed:
        model = DDP(model, device_ids=[gpu], find_unused_parameters=True)
    
    # æ•°æ®åŠ è½½å™¨
    train_loader, test_loader, train_sampler = get_resisc45_vtab_dataloaders(
        batch_size=batch_size,
        num_workers=num_workers,
        distributed=distributed,
        rank=rank,
        world_size=world_size
    )
    
    # æŸå¤±å‡½æ•° - é€‚ä¸­çš„æ ‡ç­¾å¹³æ»‘
    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)
    
    # ä¼˜åŒ–å™¨ - ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
    if distributed:
        vit_params = []
        weierstrass_params = []
        classifier_params = []
        
        for name, param in model.module.named_parameters():
            if 'weierstrass_encoding' in name:
                weierstrass_params.append(param)
            elif 'classifier' in name or 'pre_classifier' in name or 'pos_encoding_weight' in name:
                classifier_params.append(param)
            else:
                vit_params.append(param)
    else:
        vit_params = []
        weierstrass_params = []
        classifier_params = []
        
        for name, param in model.named_parameters():
            if 'weierstrass_encoding' in name:
                weierstrass_params.append(param)
            elif 'classifier' in name or 'pre_classifier' in name or 'pos_encoding_weight' in name:
                classifier_params.append(param)
            else:
                vit_params.append(param)
    
    optimizer = optim.AdamW([
        {'params': vit_params, 'lr': learning_rate * 0.1},  # é¢„è®­ç»ƒå±‚è¾ƒå°å­¦ä¹ ç‡
        {'params': weierstrass_params, 'lr': learning_rate * 1.2},  # å¨å°”æ–¯ç‰¹æ‹‰æ–¯ç¼–ç ç¨å¤§å­¦ä¹ ç‡
        {'params': classifier_params, 'lr': learning_rate * 1.8}  # åˆ†ç±»å™¨è¾ƒå¤§å­¦ä¹ ç‡
    ], weight_decay=weight_decay, betas=(0.9, 0.999))
    
    # æ¢¯åº¦ç¼©æ”¾å™¨
    scaler = GradScaler()
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç«
    def cosine_lr_lambda(epoch):
        if epoch < warmup_epochs:
            return epoch / warmup_epochs
        else:
            return 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, cosine_lr_lambda)
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
    checkpoint_dir = "/root/shared-nvme/checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    best_test_acc = 0.0
    best_epoch = 0
    results = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}
    patience = 25  # å¢åŠ patience
    patience_counter = 0
    best_model_path = os.path.join(checkpoint_dir, 'weierstrass_vit_l16_resisc45_best.pth')
    
    if rank == 0:
        print("\n=== å¼€å§‹RESISC45 VTAB-1kå¾®è°ƒå®éªŒï¼ˆ1000æ ·æœ¬è®­ç»ƒï¼Œæµ‹è¯•é›†è¯„ä¼°ï¼‰ ===")
        print(f"è®­ç»ƒæ ·æœ¬æ•°ï¼š1000 (VTAB-1k train+val)")
        print(f"æµ‹è¯•æ ·æœ¬æ•°ï¼š{len(test_loader.dataset)} (åŸæµ‹è¯•é›†)")
        print(f"ç±»åˆ«æ•°ï¼š45")
        print(f"å›¾åƒåˆ†è¾¨ç‡ï¼š{image_size}x{image_size}")
        print(f"æ¨¡å‹ï¼šViT-L/16 + å¨å°”æ–¯ç‰¹æ‹‰æ–¯ä½ç½®ç¼–ç ")
    
    for epoch in range(1, num_epochs + 1):
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch, 
            train_sampler, scaler
        )
        # æ¯ä¸€è½®éƒ½åœ¨å…¨éƒ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        test_loss, test_acc = evaluate(model, test_loader, criterion, device)
        scheduler.step()
        results['train_loss'].append(train_loss)
        results['train_acc'].append(train_acc)
        results['test_loss'].append(test_loss)
        results['test_acc'].append(test_acc)
        if rank == 0:
            print(f"\nEpoch {epoch}/{num_epochs}:")
            print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")
            print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
            # å®æ—¶ä¿å­˜æ¯è½®ç»“æœåˆ°json
            results_path = os.path.join(checkpoint_dir, 'resisc45_vtab_results.json')
            with open(results_path, 'w') as f:
                json.dump(results, f, indent=4)
            # ä¿å­˜æ¯ä¸ªepochçš„æ£€æŸ¥ç‚¹
            model_state = model.module.state_dict() if distributed else model.state_dict()
            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model_state,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'test_acc': test_acc,
                'results': results
            }, checkpoint_path)
            # ä»…å½“æµ‹è¯•é›†å‡†ç¡®ç‡è¾¾åˆ°bestæ—¶ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
            if test_acc > best_test_acc:
                best_test_acc = test_acc
                best_epoch = epoch
                patience_counter = 0
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model_state,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'best_test_acc': best_test_acc,
                    'results': results
                }, best_model_path)
                print(f"ğŸ† æ–°çš„æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
            else:
                patience_counter += 1
                if patience_counter >= patience and epoch > 40:
                    print(f"Early stopping at epoch {epoch}")
                    break
    
    # è®­ç»ƒå®Œæˆåï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    if rank == 0:
        print(f"\n=== è®­ç»ƒå®Œæˆï¼ŒåŠ è½½æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯• ===")
        if os.path.exists(best_model_path):
            checkpoint = torch.load(best_model_path, map_location=device)
            if distributed:
                model.module.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint['model_state_dict'])
            print(f"åŠ è½½ç¬¬ {checkpoint['epoch']} è½®çš„æ¨¡å‹ï¼Œæµ‹è¯•å‡†ç¡®ç‡: {checkpoint['best_test_acc']:.2f}%")
        # æœ€ç»ˆæµ‹è¯•è¯„ä¼°
        test_loss, test_acc = evaluate(model, test_loader, criterion, device)
        print(f"\n=== æœ€ç»ˆæµ‹è¯•ç»“æœ ===")
        print(f"æœ€ä½³æµ‹è¯•é›†å‡†ç¡®ç‡: {best_test_acc:.2f}% (æ¥è‡ªç¬¬{best_epoch}è½®)")
        print(f"åŸºçº¿å‡†ç¡®ç‡: 85.2%")
        if best_test_acc > 85.2:
            improvement = best_test_acc - 85.2
            print(f"ğŸ‰ æˆåŠŸè¶…è¿‡åŸºçº¿! æå‡äº† {improvement:.2f} ä¸ªç™¾åˆ†ç‚¹")
        else:
            print(f"æœªè¾¾åˆ°ç›®æ ‡ï¼Œè¿˜éœ€è¦ {85.2 - best_test_acc:.2f} ä¸ªç™¾åˆ†ç‚¹")
        # ä¿å­˜æœ€ç»ˆç»“æœ
        results['final_test_acc'] = best_test_acc
        results['best_test_epoch'] = best_epoch
        results['baseline_accuracy'] = 85.2
        results['improvement'] = best_test_acc - 85.2
        results_path = os.path.join(checkpoint_dir, 'resisc45_vtab_results.json')
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=4)
        # è¾“å‡ºå­¦ä¹ åˆ°çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‚æ•°ï¼ˆç”¨æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡çš„æ¨¡å‹ï¼‰
        if distributed:
            alpha = F.softplus(model.module.weierstrass_encoding.alpha_learn).item()
            beta = F.softplus(model.module.weierstrass_encoding.beta_learn).item()
            gamma = F.softplus(model.module.weierstrass_encoding.gamma_learn).item()
            weight = torch.sigmoid(model.module.pos_encoding_weight).item()
        else:
            alpha = F.softplus(model.weierstrass_encoding.alpha_learn).item()
            beta = F.softplus(model.weierstrass_encoding.beta_learn).item()
            gamma = F.softplus(model.weierstrass_encoding.gamma_learn).item()
            weight = torch.sigmoid(model.pos_encoding_weight).item()
        print(f"\nå­¦ä¹ åˆ°çš„å¨å°”æ–¯ç‰¹æ‹‰æ–¯å‚æ•°:")
        print(f"Ï‰'2 (alpha): {alpha:.6f}")
        print(f"Î² (beta): {beta:.6f}")
        print(f"Î³ (gamma): {gamma:.6f}")
        print(f"ä½ç½®ç¼–ç æ··åˆæƒé‡: {weight:.6f}")
        print(f"\n=== å®éªŒæ€»ç»“ ===")
        print(f"VTAB-1k RESISC45é…ç½®:")
        print(f"- è®­ç»ƒæ ·æœ¬: 1000 (VTAB-1k train+val)")
        print(f"- æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset)}")
        print(f"- ç±»åˆ«æ•°: 45")
        print(f"- æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
        print(f"- åŸºçº¿å‡†ç¡®ç‡: 85.2%")
        print(f"- æå‡å¹…åº¦: {best_test_acc - 85.2:.2f} ä¸ªç™¾åˆ†ç‚¹")
    
    if distributed:
        dist.destroy_process_group()

if __name__ == "__main__":
    main()