import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Tuple, Dict
import os
import json
from tqdm import tqdm
import random
from W_Ti import ImprovedViT_Ti
import matplotlib
from matplotlib import font_manager


def _setup_chinese_font():
    """
    ä¸ºMatplotlibè®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œä¼˜å…ˆæŒ‰å€™é€‰åˆ—è¡¨è‡ªåŠ¨é€‰æ‹©å·²å®‰è£…å­—ä½“ã€‚
    è‹¥å‡ä¸å¯ç”¨ï¼Œåˆ™ä¿æŒé»˜è®¤å­—ä½“å¹¶å°½é‡é¿å…è´Ÿå·æ˜¾ç¤ºé—®é¢˜ã€‚
    """
    candidate_families = [
        'Noto Sans CJK SC', 'Noto Sans CJK', 'Noto Sans SC',
        'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei',
        'Source Han Sans SC', 'Source Han Sans CN', 'PingFang SC'
    ]
    chosen = None
    for fam in candidate_families:
        try:
            font_manager.FontProperties(family=fam)
            # findfontä¼šåœ¨æ‰¾ä¸åˆ°æ—¶æŠ›å‡ºå¼‚å¸¸ï¼ˆfallback_to_default=Falseï¼‰
            font_path = font_manager.findfont(
                font_manager.FontProperties(family=fam), fallback_to_default=False
            )
            if os.path.exists(font_path):
                chosen = fam
                break
        except Exception:
            continue
    if chosen is not None:
        matplotlib.rcParams['font.sans-serif'] = [chosen, 'DejaVu Sans']
    else:
        # æ— å¯ç”¨ä¸­æ–‡å­—ä½“æ—¶ä¿ç•™é»˜è®¤ï¼Œä½†ä»è®¾ç½®å¤‡é€‰ä»¥å‡å°‘å‘Šè­¦
        matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
    # é¿å…è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—
    matplotlib.rcParams['axes.unicode_minus'] = False


def _apply_publication_style():
    """Apply clean publication-style Matplotlib rcParams (English labels)."""
    matplotlib.rcParams.update({
        'font.family': 'DejaVu Sans',
        'figure.dpi': 120,
        'savefig.dpi': 300,
        'axes.titlesize': 14,
        'axes.labelsize': 12,
        'axes.linewidth': 1.2,
        'axes.edgecolor': '#222222',
        'axes.grid': True,
        'grid.color': '#AAAAAA',
        'grid.alpha': 0.25,
        'grid.linestyle': '-',
        'xtick.labelsize': 11,
        'ytick.labelsize': 11,
        'xtick.direction': 'out',
        'ytick.direction': 'out',
        'xtick.major.size': 5,
        'ytick.major.size': 5,
        'legend.frameon': True,
        'legend.fancybox': True,
        'legend.framealpha': 0.9,
        'legend.borderpad': 0.6,
    })

# å‡è®¾åŸå§‹ä»£ç å·²ç»å¯¼å…¥ï¼Œè¿™é‡Œåªæ·»åŠ ç¼ºå¤±çš„éƒ¨åˆ†

class OcclusionTransform:
    """
    éšæœºé®æŒ¡å˜æ¢ï¼Œæ”¯æŒä¸åŒçš„é®æŒ¡å¼ºåº¦
    """
    def __init__(self, occlusion_ratio: float = 0.1, fill_value: float = 0.0):
        """
        Args:
            occlusion_ratio: é®æŒ¡é¢ç§¯æ¯”ä¾‹ (0.0-1.0)
            fill_value: é®æŒ¡åŒºåŸŸå¡«å……å€¼
        """
        self.occlusion_ratio = occlusion_ratio
        self.fill_value = fill_value
    
    def __call__(self, img):
        """
        å¯¹è¾“å…¥å›¾åƒåº”ç”¨éšæœºé®æŒ¡
        Args:
            img: PIL Image æˆ– tensor
        Returns:
            é®æŒ¡åçš„å›¾åƒ
        """
        if isinstance(img, torch.Tensor):
            c, h, w = img.shape
        else:
            # PIL Image
            w, h = img.size
            c = 3
        
        # è®¡ç®—é®æŒ¡å—å¤§å°
        total_area = h * w
        occlusion_area = int(total_area * self.occlusion_ratio)
        
        # ç”Ÿæˆå¤šä¸ªå°çš„é®æŒ¡å—è€Œä¸æ˜¯å•ä¸ªå¤§å—ï¼Œæ›´æ¥è¿‘å®é™…åœºæ™¯
        num_blocks = random.randint(1, max(1, int(self.occlusion_ratio * 20)))
        
        if isinstance(img, torch.Tensor):
            occluded_img = img.clone()
        else:
            occluded_img = transforms.ToTensor()(img)
        
        # åº”ç”¨å¤šä¸ªé®æŒ¡å—
        total_occluded = 0
        for _ in range(num_blocks):
            if total_occluded >= occlusion_area:
                break
                
            # éšæœºç”Ÿæˆé®æŒ¡å—å¤§å°
            remaining_area = occlusion_area - total_occluded
            block_area = min(remaining_area, random.randint(1, remaining_area + 1))
            
            # è®¡ç®—é®æŒ¡å—çš„é•¿å®½æ¯”
            aspect_ratio = random.uniform(0.5, 2.0)  # é•¿å®½æ¯”åœ¨0.5-2.0ä¹‹é—´
            block_h = max(1, int(np.sqrt(block_area / aspect_ratio)))
            block_w = max(1, int(block_area / block_h))
            
            # ç¡®ä¿ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
            block_h = max(1, min(block_h, h))
            block_w = max(1, min(block_w, w))
            
            # éšæœºé€‰æ‹©é®æŒ¡ä½ç½®
            start_h = random.randint(0, max(0, h - block_h))
            start_w = random.randint(0, max(0, w - block_w))
            
            # åº”ç”¨é®æŒ¡
            occluded_img[:, start_h:start_h + block_h, start_w:start_w + block_w] = self.fill_value
            total_occluded += block_h * block_w
        
        return occluded_img


def create_baseline_vit_ti():
    """åˆ›å»ºä½¿ç”¨APEçš„åŸºçº¿ViT-Tiæ¨¡å‹"""
    model = ImprovedViT_Ti(
        img_size=224,
        patch_size=16,
        in_channels=3,
        num_classes=100,
        embed_dim=192,
        depth=12,
        num_heads=3,
        mlp_ratio=4.0,
        dropout=0.05,
        drop_path=0.05,
        use_wef=False  # ä½¿ç”¨æ ‡å‡†APE
    )
    return model


def evaluate_model_with_occlusion(model, dataloader, device, occlusion_ratios):
    """
    è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé®æŒ¡ç‡ä¸‹çš„æ€§èƒ½
    
    Args:
        model: å¾…è¯„ä¼°çš„æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡
        occlusion_ratios: é®æŒ¡ç‡åˆ—è¡¨
    
    Returns:
        results: åŒ…å«ä¸åŒé®æŒ¡ç‡ä¸‹å‡†ç¡®ç‡çš„å­—å…¸
    """
    model.eval()
    results = {}
    
    with torch.no_grad():
        for occlusion_ratio in tqdm(occlusion_ratios, desc="è¯„ä¼°ä¸åŒé®æŒ¡ç‡"):
            correct = 0
            total = 0
            
            for batch_idx, (images, labels) in enumerate(tqdm(dataloader, leave=False, desc=f"é®æŒ¡ç‡ {occlusion_ratio:.1%}")):
                # å¯¹æ¯ä¸ªbatchåº”ç”¨é®æŒ¡
                if occlusion_ratio > 0:
                    occluded_images = []
                    for img in images:
                        # å°†tensorè½¬æ¢ä¸ºPILè¿›è¡Œå¤„ç†ï¼Œç„¶åå†è½¬å›tensor
                        # å…ˆåå½’ä¸€åŒ–
                        mean = torch.tensor([0.5071, 0.4865, 0.4409]).view(3, 1, 1)
                        std = torch.tensor([0.2673, 0.2564, 0.2762]).view(3, 1, 1)
                        unnorm_img = img * std + mean
                        unnorm_img = torch.clamp(unnorm_img, 0, 1)
                        
                        # åº”ç”¨é®æŒ¡
                        occlusion_transform = OcclusionTransform(occlusion_ratio, fill_value=0.0)
                        occluded_img = occlusion_transform(unnorm_img)
                        
                        # é‡æ–°å½’ä¸€åŒ–
                        occluded_img = (occluded_img - mean) / std
                        occluded_images.append(occluded_img)
                    
                    images = torch.stack(occluded_images)
                
                images, labels = images.to(device), labels.to(device)
                
                outputs = model(images)
                _, predicted = outputs.max(1)
                
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                # ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œå¯ä»¥é™åˆ¶è¯„ä¼°çš„batchæ•°é‡
                if batch_idx >= 50:  # åªè¯„ä¼°å‰50ä¸ªbatch
                    break
            
            accuracy = 100.0 * correct / total
            results[occlusion_ratio] = accuracy
            print(f"é®æŒ¡ç‡ {occlusion_ratio:.1%}: å‡†ç¡®ç‡ = {accuracy:.2f}%")
    
    return results


def plot_occlusion_robustness_comparison(wef_results, ape_results, save_path='occlusion_robustness.png'):
    """
    Plot occlusion robustness comparison (English labels)
    """
    _apply_publication_style()
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.8))
    
    occlusion_ratios = sorted(wef_results.keys())
    wef_accuracies = [wef_results[ratio] for ratio in occlusion_ratios]
    ape_accuracies = [ape_results[ratio] for ratio in occlusion_ratios]
    
    # Accuracy curves
    x_vals = [r*100 for r in occlusion_ratios]
    ax1.plot(x_vals, wef_accuracies, marker='o', linestyle='-', linewidth=2.2, markersize=6,
             color='#2E86AB', label='WEF-PE (Ours)')
    ax1.plot(x_vals, ape_accuracies, marker='s', linestyle='--', linewidth=2.0, markersize=6,
             color='#A23B72', label='APE (Baseline)')
    
    ax1.set_xlabel('Occlusion (%)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')
    ax1.set_title('Accuracy vs Occlusion Ratio', fontsize=14, fontweight='bold')
    ax1.legend(loc='best', fontsize=10, ncol=1)
    ax1.grid(True, alpha=0.25)
    y_min = min(min(wef_accuracies), min(ape_accuracies))
    y_max = max(max(wef_accuracies), max(ape_accuracies))
    ax1.set_ylim([max(0, y_min - 3), y_max + 3])
    ax1.set_xticks(x_vals)
    
    # Value annotations
    for i, (ratio, wef_acc, ape_acc) in enumerate(zip(occlusion_ratios, wef_accuracies, ape_accuracies)):
        ax1.annotate(f'{wef_acc:.1f}', (ratio*100, wef_acc),
                    textcoords="offset points", xytext=(0,8), ha='center', fontsize=9, color='#2E86AB')
        ax1.annotate(f'{ape_acc:.1f}', (ratio*100, ape_acc),
                    textcoords="offset points", xytext=(0,-12), ha='center', fontsize=9, color='#A23B72')
    
    # Degradation curves
    wef_baseline = wef_accuracies[0]
    ape_baseline = ape_accuracies[0]
    
    wef_degradation = [(wef_baseline - acc) for acc in wef_accuracies]
    ape_degradation = [(ape_baseline - acc) for acc in ape_accuracies]
    
    ax2.plot(x_vals, wef_degradation, marker='o', linestyle='-', linewidth=2.2, markersize=6,
             color='#2E86AB', label='WEF-PE (Ours)')
    ax2.plot(x_vals, ape_degradation, marker='s', linestyle='--', linewidth=2.0, markersize=6,
             color='#A23B72', label='APE (Baseline)')
    
    ax2.set_xlabel('Occlusion (%)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Degradation (pp)', fontsize=12, fontweight='bold')
    ax2.set_title('Performance Degradation', fontsize=14, fontweight='bold')
    ax2.legend(loc='best', fontsize=10, ncol=1)
    ax2.grid(True, alpha=0.25)
    ax2.set_xticks(x_vals)
    
    # Value annotations for B panel (match A panel style/colors)
    for ratio, w_deg, a_deg in zip(occlusion_ratios, wef_degradation, ape_degradation):
        ax2.annotate(f'{w_deg:.1f}', (ratio*100, w_deg),
                     textcoords="offset points", xytext=(0,8), ha='center', fontsize=9, color='#2E86AB')
        ax2.annotate(f'{a_deg:.1f}', (ratio*100, a_deg),
                     textcoords="offset points", xytext=(0,-12), ha='center', fontsize=9, color='#A23B72')

    # Improvement annotations
    for i, (ratio, wef_deg, ape_deg) in enumerate(zip(occlusion_ratios[1:], wef_degradation[1:], ape_degradation[1:])):
        if ape_deg > wef_deg:
            improvement = ape_deg - wef_deg
            ax2.annotate(f'+{improvement:.1f} pp', 
                        (ratio*100, (wef_deg + ape_deg) / 2), 
                        textcoords="offset points", xytext=(10,0), ha='left', 
                        fontsize=9, color='green', fontweight='bold')

    # Panel labels
    ax1.text(-0.1, 1.05, 'A', transform=ax1.transAxes, fontsize=14, fontweight='bold', va='top')
    ax2.text(-0.1, 1.05, 'B', transform=ax2.transAxes, fontsize=14, fontweight='bold', va='top')
    
    plt.tight_layout()
    # Save high-quality images
    base, ext = os.path.splitext(save_path)
    plt.savefig(f'{base}.png', dpi=300, bbox_inches='tight')
    try:
        plt.savefig(f'{base}.pdf', bbox_inches='tight')
    except Exception:
        pass
    print(f"å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.show()


def create_test_dataloader_cifar100(batch_size=64, num_workers=4):
    """åˆ›å»ºCIFAR-100æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],
                           std=[0.2673, 0.2564, 0.2762])
    ])
    
    test_dataset = torchvision.datasets.CIFAR100(
        root='/root/shared-nvme', train=False, download=False, transform=test_transform
    )
    
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True
    )
    
    return test_loader


def load_trained_models(wef_model_path='best_wef_vit_ti.pth', device='cuda'):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        wef_model_path: WEFæ¨¡å‹æƒé‡è·¯å¾„
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        wef_model, ape_model: åŠ è½½å¥½çš„æ¨¡å‹
    """
    # åˆ›å»ºWEFæ¨¡å‹å¹¶åŠ è½½æƒé‡
    wef_model = ImprovedViT_Ti(
        img_size=224,
        patch_size=16,
        in_channels=3,
        num_classes=100,
        embed_dim=192,
        depth=12,
        num_heads=3,
        mlp_ratio=4.0,
        dropout=0.05,
        drop_path=0.05,
        use_wef=True
    )
    
    if os.path.exists(wef_model_path):
        print(f"åŠ è½½WEFæ¨¡å‹æƒé‡: {wef_model_path}")
        wef_model.load_state_dict(torch.load(wef_model_path, map_location=device))
    else:
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°WEFæ¨¡å‹æƒé‡æ–‡ä»¶ {wef_model_path}")
        print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è¿›è¡Œæ¼”ç¤º")
    
    wef_model = wef_model.to(device)
    
    # ä¸ºäº†å¯¹æ¯”ï¼Œæˆ‘ä»¬éœ€è¦è®­ç»ƒä¸€ä¸ªAPEåŸºçº¿æ¨¡å‹
    # è¿™é‡Œæˆ‘ä»¬å‡è®¾ä½ ä¹Ÿæœ‰ä¸€ä¸ªAPEæ¨¡å‹çš„æƒé‡æ–‡ä»¶
    ape_model_path = 'best_ape_vit_ti.pth'  # å‡è®¾çš„APEæ¨¡å‹è·¯å¾„
    
    ape_model = ImprovedViT_Ti(
        img_size=224,
        patch_size=16,
        in_channels=3,
        num_classes=100,
        embed_dim=192,
        depth=12,
        num_heads=3,
        mlp_ratio=4.0,
        dropout=0.05,
        drop_path=0.05,
        use_wef=False  # ä½¿ç”¨APE
    )
    
    if os.path.exists(ape_model_path):
        print(f"åŠ è½½APEæ¨¡å‹æƒé‡: {ape_model_path}")
        ape_model.load_state_dict(torch.load(ape_model_path, map_location=device))
    else:
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°APEæ¨¡å‹æƒé‡æ–‡ä»¶ {ape_model_path}")
        print("å°†è®­ç»ƒä¸€ä¸ªå¿«é€Ÿçš„APEåŸºçº¿æ¨¡å‹è¿›è¡Œå¯¹æ¯”")
        ape_model = train_ape_baseline_quickly(ape_model, device)
    
    ape_model = ape_model.to(device)
    
    return wef_model, ape_model


def train_ape_baseline_quickly(model, device, epochs=20):
    """
    å¿«é€Ÿè®­ç»ƒä¸€ä¸ªAPEåŸºçº¿æ¨¡å‹ç”¨äºå¯¹æ¯”
    æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è®­ç»ƒè¿‡ç¨‹ï¼Œä¸»è¦ç”¨äºæ¼”ç¤º
    """
    print("å¼€å§‹å¿«é€Ÿè®­ç»ƒAPEåŸºçº¿æ¨¡å‹...")
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],
                           std=[0.2673, 0.2564, 0.2762])
    ])
    
    train_dataset = torchvision.datasets.CIFAR100(
        root='/root/shared-nvme', train=True, download=False, transform=train_transform
    )
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True
    )
    
    model = model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.05)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')
        for batch_idx, (images, labels) in enumerate(pbar):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            if batch_idx % 50 == 0:
                acc = 100.0 * correct / total
                pbar.set_postfix({'Loss': f'{running_loss/(batch_idx+1):.3f}', 'Acc': f'{acc:.2f}%'})
            
            # ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œé™åˆ¶æ¯ä¸ªepochçš„batchæ•°é‡
            if batch_idx >= 100:
                break
    
    # ä¿å­˜å¿«é€Ÿè®­ç»ƒçš„æ¨¡å‹
    torch.save(model.state_dict(), 'best_ape_vit_ti.pth')
    print("APEåŸºçº¿æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜")
    
    return model


def run_occlusion_robustness_experiment():
    """
    è¿è¡Œå®Œæ•´çš„é®æŒ¡é²æ£’æ€§å¯¹æ¯”å®éªŒ
    """
    print("=" * 60)
    print("WEF-PE vs APE é®æŒ¡é²æ£’æ€§å¯¹æ¯”å®éªŒ")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    print("åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨...")
    test_loader = create_test_dataloader_cifar100(batch_size=32, num_workers=4)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    print("åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    wef_model, ape_model = load_trained_models(device=device)
    
    # å®šä¹‰é®æŒ¡ç‡
    occlusion_ratios = [0.0, 0.1, 0.2, 0.3]
    print(f"æµ‹è¯•é®æŒ¡ç‡: {[f'{r:.1%}' for r in occlusion_ratios]}")
    
    # è¯„ä¼°WEFæ¨¡å‹
    print("\nè¯„ä¼°WEF-PEæ¨¡å‹...")
    wef_results = evaluate_model_with_occlusion(wef_model, test_loader, device, occlusion_ratios)
    
    # è¯„ä¼°APEåŸºçº¿æ¨¡å‹
    print("\nè¯„ä¼°APEåŸºçº¿æ¨¡å‹...")
    ape_results = evaluate_model_with_occlusion(ape_model, test_loader, device, occlusion_ratios)
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("\n" + "=" * 50)
    print("å®éªŒç»“æœæ€»ç»“")
    print("=" * 50)
    print(f"{'é®æŒ¡ç‡':<10} {'WEF-PE':<10} {'APE':<10} {'æ”¹å–„':<10}")
    print("-" * 40)
    
    for ratio in occlusion_ratios:
        wef_acc = wef_results[ratio]
        ape_acc = ape_results[ratio]
        improvement = wef_acc - ape_acc
        print(f"{ratio:>6.1%}   {wef_acc:>7.2f}%   {ape_acc:>6.2f}%   {improvement:>+6.2f}pp")
    
    # è®¡ç®—å¹³å‡æ”¹å–„
    improvements = [wef_results[r] - ape_results[r] for r in occlusion_ratios[1:]]  # æ’é™¤æ— é®æŒ¡æƒ…å†µ
    avg_improvement = np.mean(improvements)
    print(f"\nå¹³å‡æ”¹å–„: {avg_improvement:+.2f} ä¸ªç™¾åˆ†ç‚¹")
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    print("\nç»˜åˆ¶å¯¹æ¯”å›¾...")
    plot_occlusion_robustness_comparison(wef_results, ape_results)
    
    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    results_dict = {
        'wef_results': wef_results,
        'ape_results': ape_results,
        'occlusion_ratios': occlusion_ratios,
        'average_improvement': float(avg_improvement)
    }
    
    with open('occlusion_robustness_results.json', 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=2, ensure_ascii=False)
    
    print("å®éªŒç»“æœå·²ä¿å­˜åˆ°: occlusion_robustness_results.json")
    print("\nå®éªŒå®Œæˆ! ğŸ‰")
    
    return results_dict


if __name__ == "__main__":
    results = run_occlusion_robustness_experiment()